{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6b4bd5-8373-42cc-84d9-31b5860f7655",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://erachelson.github.io/RLclass_MVA/\">https://erachelson.github.io/RLclass_MVA/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20521c9-fe11-4dbc-b77d-42561a96b7e1",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Chapter 4: From fitted Q-iteration to deep Q-networks</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9092fe-8dcc-4fbc-a7c1-df9075090aa6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Learning outcomes**   \n",
    "By the end of this chapter, you should be able to:\n",
    "- write and execute a fitted Q-iteration algorithm with any offline regression method,\n",
    "- explain the limitations of fitted Q-iteration,\n",
    "- implement the deep Q-networks architecture(s) and algorithm, and their immediate improvements with target networks, gradient clipping and alternate optimizers,\n",
    "- discuss the limitations of DQN, list improvements present in Rainbow, and cite their sources.\n",
    "\n",
    "Additionally, after doing the homework(*), you should be able to:\n",
    "- explain and implement n-step returns\n",
    "- explain and implement double DQN\n",
    "- explain and implement prioritized experience replay\n",
    "- explain and implement C51\n",
    "- explain and implement NoisyNets\n",
    "- explain and implement Munchausen DQN\n",
    "\n",
    "(*) as of Nov 13th, 2024, this part is not implemented yet.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaf67c3-9048-4c34-a6db-e15c260c3828",
   "metadata": {},
   "source": [
    "In this second series of chapters, we confront our foundational knowledge to various challenges in RL: continuous, sometimes high-dimensional state spaces, sparse rewards, difficult exploration with deceptive rewards, continuous actions, etc. Most problems will combine several of these challenges. We will take an algorithmic approach, building modern RL algorithms as solutions to these challenges.\n",
    "\n",
    "In this chapter, we build upon the foundations of approximate value iteration (AVI), which we studied in depth previously and which enabled designing Q-learning. We depart from toy problems and dive into larger scale ones, with the intention to build approximate value iteration algorithms that scale to challenging environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281a9cab-f286-4f64-b994-f65cbdd791d6",
   "metadata": {},
   "source": [
    "# Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87685ab1-6fb1-4007-ad17-8708218ed524",
   "metadata": {},
   "source": [
    "## Cart-pole\n",
    "\n",
    "Full description on the [gymnasium website](https://gymnasium.farama.org/environments/classic_control/cart_pole/).\n",
    "\n",
    "> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a7d443-a9e4-491a-ba10-c5fd2090db3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "cartpole = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "print(\"Observation space:\", cartpole.observation_space)\n",
    "print(\"Action space:\", cartpole.action_space)\n",
    "s,_ = cartpole.reset()\n",
    "print(\"Starting state:\", s)\n",
    "plt.imshow(cartpole.render());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1066cf9-d586-46e3-a89a-cacb6da07e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "cartpole = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "s,_ = cartpole.reset()\n",
    "for t in range(200):\n",
    "    s2,r,d,trunc,_ = cartpole.step(cartpole.action_space.sample())\n",
    "    cartpole.render()\n",
    "    if d:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b0e279-47b3-492c-b117-1b7daf67be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73b4deb-e2d4-46e2-954e-8adb6cd8261a",
   "metadata": {},
   "source": [
    "## Acrobot\n",
    "\n",
    "Full description on the [gymnasium website](https://gymnasium.farama.org/environments/classic_control/acrobot/).\n",
    "\n",
    "> The system consists of two links connected linearly to form a chain, with one end of the chain fixed. The joint between the two links is actuated. The goal is to apply torques on the actuated joint to swing the free end of the linear chain above a given height while starting from the initial state of hanging downwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fe2681-d80b-44e3-9412-682531b9a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "acrobot = gym.make('Acrobot-v1', render_mode=\"rgb_array\")\n",
    "print(\"Observation space:\", acrobot.observation_space)\n",
    "print(\"Action space:\", acrobot.action_space)\n",
    "s,_ = acrobot.reset()\n",
    "print(\"Starting state:\", s)\n",
    "plt.imshow(acrobot.render());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc192ed-8ebb-43e3-96a7-84e0ea3a7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "acrobot = gym.make('Acrobot-v1', render_mode=\"human\")\n",
    "s,_ = acrobot.reset()\n",
    "for t in range(200):\n",
    "    s2,r,d,trunc,_ = acrobot.step(acrobot.action_space.sample())\n",
    "    acrobot.render()\n",
    "    if d:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54aecf0-3e20-4d6b-8f80-80747ef528ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "acrobot.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b710f1df-69c1-4160-b8c8-3ce2ef009644",
   "metadata": {},
   "source": [
    "## Pong\n",
    "\n",
    "Let's build an agent that learns to play Pong, one of the [Atari games](https://gymnasium.farama.org/environments/atari/pong/) in Gymnasium (originally in the [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment)). You might want to try different games later on (like the popular Breakout game for instance).\n",
    "\n",
    "Here is the environment's description.\n",
    "> Maximize your score in the Atari 2600 game Pong. In this environment, the observation is an RGB image of the screen, which is an array of shape (210, 160, 3). Each action is repeatedly performed for a duration of k frames, where k is uniformly sampled from $\\{2, 3, 4\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8084282e-d02c-4696-b1da-2e6d25b7c21f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "pong = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "print(\"Observation space:\", pong.observation_space)\n",
    "print(\"Action space:\", pong.action_space)\n",
    "s,_ = pong.reset()\n",
    "plt.imshow(pong.render(),vmin=0,vmax=255);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a5edd8-d23a-4da3-9aa7-4387ed637d2f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:** What is the number of possible states? Why is this not an MDP? What would one need to turn this back into an MDP?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76da9e1-18bf-4ac8-a28e-67c81edd36a1",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "One frame is a $210\\times 160$ RGB image with a 256 color palette, so the set of all possible frames has size $256^{210 \\times 160 \\times 3} \\sim 10^{242579}$. That's a little too many for an efficient enumeration. Of course, most of the possible images will never occur in a Breakout game and the true state space is actually a much smaller subset of the full set of possible images. Nevertheless, unless we provide a large engineering effort in describing the state space with few variables (which would be contradictory of our goal of a \"human-level\" AI) we will need to automatically discover some structure in the state sampled data.\n",
    "\n",
    "This is not an MDP because the transition dynamics do not respect Markov's property. The probability of transitioning from $s_t$ to $s_{t+1}$ is *not* independent of previous states. The problem here is that a single frame of the game does not reflect the velocity of the ball.\n",
    "\n",
    "To recover Markov's property one could simply stack a few frames together in the state space.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f56c0d-e8f3-43cd-9882-724cc4a4c4f0",
   "metadata": {},
   "source": [
    "There are 18 buttons on the Atari controller. However not all games use all buttons. Our interface to Pong specifies 6 possible actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb70b2-c1ba-46f6-bdda-3b767ee685ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pong.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d48d5e4-c22a-40f0-b048-2e4d5ccd15cd",
   "metadata": {},
   "source": [
    "Gymnasium provides us with a number of environment wrappers that help preprocessing observations, actions or environment-specific features. There is [one such wrapper for Atari games](https://gymnasium.farama.org/api/wrappers/misc_wrappers/#gymnasium.wrappers.AtariPreprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d4c732-1cda-437c-b4b8-2c4b97bcd0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import AtariPreprocessing\n",
    "pong = AtariPreprocessing(pong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3195df17-3bf5-4e17-b2a9-30fe08dfa111",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pong.observation_space)\n",
    "print(pong.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41111b18-8ce8-4ed9-a062-35b71972530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying a random agent in Pong\n",
    "import time\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_,_ = pong.reset()\n",
    "for i in range(60):\n",
    "    a = np.random.randint(2)\n",
    "    x, r, _, _, _ = pong.step(a)\n",
    "    \n",
    "print(\"shape: \", x.shape, \", min = \", x.min(), \", max = \", x.max(), \", dtype = \", x.dtype, sep='')\n",
    "plt.imshow(x, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08e737-c445-4240-9149-e746f4c9dc58",
   "metadata": {},
   "source": [
    "# Fitted Q-iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02291ecc-40f4-4d2f-9b1d-75592100a6f8",
   "metadata": {},
   "source": [
    "## Reminder: AVI as a series of risk minimization problems\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Approximate value iteration as a sequence of risk minimization problems**  \n",
    "$$\\pi_n \\in \\mathcal{G} q_n,$$\n",
    "$$L_n(\\theta) = \\frac{1}{2} \\mathbb{E}_{(s,a) \\sim \\rho}\\left[ \\left( q(s,a;\\theta) - G^{\\pi_n}_1(s,a,q_n) \\right)^2 \\right],$$\n",
    "$$\\theta_{n+1} \\in \\arg\\min_{\\theta} L_n(\\theta),$$\n",
    "$$q_{n+1}(s,a) = q(s,a;\\theta_{n+1}).$$\n",
    "</div>\n",
    "\n",
    "With \n",
    "$$\\pi \\in \\mathcal{G} q \\quad \\Leftrightarrow \\quad \\pi(s) \\in \\arg\\max_{\\pi \\in \\Delta_A} \\mathbb{E}_{a\\sim\\pi} \\left[q(s,a)\\right]$$\n",
    "\n",
    "And\n",
    "$$G^\\pi_1(s,a,q) = R_0 + \\gamma q(S_1, A_1) \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a\\\\ A_1 \\sim \\pi(S_1),\\\\ S_{1}\\sim p(\\cdot|S_0,A_0),\\\\ R_0 = r(S_0,A_0,S_{1}).\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a056402-7ce6-48cb-a1fd-d0fba2f97d44",
   "metadata": {},
   "source": [
    "In the previous chapter, we saw that the exact, tabular representation of $q$ was actually a linear model. In turn, this opened the possibility to use stochastic approximation to solve the $\\min_\\theta L_n(\\theta)$ problem, leading to Q-learning and SARSA.\n",
    "\n",
    "Such representations do not scale to large state spaces, and we know we should turn to function approximation sooner or later. \n",
    "Given iid samples $(s,a,r,s')$, solving $\\min_\\theta L_n(\\theta)$ translates to a supervised learning problem whose inputs $x$ are $x=(s,a)$ pairs, and outputs $y$ are $y=r+\\gamma \\mathbb{E}_{a'\\sim \\pi_n(s')} q_n(s',a')$.  \n",
    "\n",
    "While, in the previous chapter, stochastic approximation enabled learning $\\min_\\theta L_n(\\theta)$ with on-the-fly acquired samples (provided the function approximator was differentiable), it seems reasonable to enable storage of samples, leading to a training set of iid samples $\\{(s,a,r,s')\\}$ which we called an *experience replay memory* or *replay buffer*: with this, the regression problem now has a function representation space and a training set of input-output pairs $\\{(x,y)\\}$.\n",
    "\n",
    "The supervised learning literature provides us with a span of efficient methods to solve such a regression problem. \n",
    "Random forests is a reasonably all-purpose method, with great properties in many cases: let's build a sample-based AVI method using random forests.\n",
    "\n",
    "In general, the use of supervised learning regressors as approximators of the sequence of AVI's Q-functions is called **fitted Q-iteration (FQI)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f84a1-c9aa-44a6-8fa8-af1a4c7c2408",
   "metadata": {},
   "source": [
    "## A random forest based FQI algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b4cdfb-c5e5-4219-a2d6-ff8ff835ac70",
   "metadata": {},
   "source": [
    "One often considers that the two historical papers implementing FQI algorithms are the ones describing **[Tree-Based Batch Mode Reinforcement Learning](https://www.jmlr.org/papers/volume6/ernst05a/ernst05a.pdf)** (Ernst et al., 2005) and **[Neural Fitted Q Iteration](https://link.springer.com/chapter/10.1007/11564096_32)** (Riedmiller, 2005). Of course they were not the only ones but they are often quoted as milestones in RL history.\n",
    "\n",
    "We are going to implement an algorithm which is very close to that presented in the Tree-Based Batch Mode Reinforcement Learning paper. The main difference is that the authors employed a variant of random forests called extra trees. Scikit-learn provides an implementation of both random forests and extra trees, feel free to swap one for the other in what follows. \n",
    "\n",
    "The first step to perform FQI is to collect and store a dataset of samples. \n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Collect $10^4$ samples using a uniformly random policy and store them.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5113f20a-2a02-4160-b519-18944b721785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def collect_samples(env, horizon, disable_tqdm=False, print_done_states=False):\n",
    "    s, _ = env.reset()\n",
    "    #dataset = []\n",
    "    S = []\n",
    "    A = []\n",
    "    R = []\n",
    "    S2 = []\n",
    "    D = []\n",
    "    for _ in tqdm(range(horizon), disable=disable_tqdm):\n",
    "        a = env.action_space.sample()\n",
    "        s2, r, done, trunc, _ = env.step(a)\n",
    "        #dataset.append((s,a,r,s2,done,trunc))\n",
    "        S.append(s)\n",
    "        A.append(a)\n",
    "        R.append(r)\n",
    "        S2.append(s2)\n",
    "        D.append(done)\n",
    "        if done or trunc:\n",
    "            s, _ = env.reset()\n",
    "            if done and print_done_states:\n",
    "                print(\"done!\")\n",
    "        else:\n",
    "            s = s2\n",
    "    S = np.array(S)\n",
    "    A = np.array(A).reshape((-1,1))\n",
    "    R = np.array(R)\n",
    "    S2= np.array(S2)\n",
    "    D = np.array(D)\n",
    "    return S, A, R, S2, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b35f56-99be-43ee-a294-4023bb641959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "cartpole = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "S,A,R,S2,D = collect_samples(cartpole, int(1e4))\n",
    "print(\"nb of collected samples:\", S.shape[0])\n",
    "for i in range(3):\n",
    "    print(\"sample\", i, \"\\n  state:\", S[i], \"\\n  action:\", A[i], \"\\n  reward:\", R[i], \"\\n  next state:\", S2[i], \"\\n terminal?\", D[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63019e11-3db5-4a22-819a-b978ede5244f",
   "metadata": {},
   "source": [
    "Now let's consider $q_0$ is the constant zero function and train a random forest to learn $q_1$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Construct the training set to learn $q_1$, then train it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b13a15-f58c-4e97-8543-4d452bd5276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "SA = np.append(S,A,axis=1)\n",
    "value = R.copy()\n",
    "\n",
    "Q1 = RandomForestRegressor()\n",
    "Q1.fit(SA,value);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dad256b-eb24-4451-8f6a-361f388026a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute MSE\n",
    "print(\"training MSE:\", np.mean((value-Q1.predict(SA))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddd3ee3-f25d-4abc-9d12-8feef90a5332",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Restart from scratch and write the loop that builds the sequence of AVI Q-functions. Store all successive Q-functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a11e2-8400-4406-8ed0-89fcb64b1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/rf_fqi.py\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "def rf_fqi(S, A, R, S2, D, iterations, nb_actions, gamma, disable_tqdm=False):\n",
    "    nb_samples = S.shape[0]\n",
    "    Qfunctions = []\n",
    "    SA = np.append(S,A,axis=1)\n",
    "    for iter in tqdm(range(iterations), disable=disable_tqdm):\n",
    "        if iter==0:\n",
    "            value=R.copy()\n",
    "        else:\n",
    "            Q2 = np.zeros((nb_samples,nb_actions))\n",
    "            for a2 in range(nb_actions):\n",
    "                A2 = a2*np.ones((S.shape[0],1))\n",
    "                S2A2 = np.append(S2,A2,axis=1)\n",
    "                Q2[:,a2] = Qfunctions[-1].predict(S2A2)\n",
    "            max_Q2 = np.max(Q2,axis=1)\n",
    "            value = R + gamma*(1-D)*max_Q2\n",
    "        Q = RandomForestRegressor()\n",
    "        Q.fit(SA,value)\n",
    "        Qfunctions.append(Q)\n",
    "    return Qfunctions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea5307-3cc6-4ac3-b5d9-10413be5b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = .9\n",
    "nb_iter = 10\n",
    "nb_actions = cartpole.action_space.n\n",
    "Qfunctions = rf_fqi(S, A, R, S2, D, nb_iter, nb_actions, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb16007-61f4-46aa-a909-dcda4d884170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Value of an initial state across iterations\n",
    "s0,_ = cartpole.reset()\n",
    "Vs0 = np.zeros(nb_iter)\n",
    "for i in range(nb_iter):\n",
    "    Qs0a = []\n",
    "    for a in range(cartpole.action_space.n):\n",
    "        s0a = np.append(s0,a).reshape(1, -1)\n",
    "        Qs0a.append(Qfunctions[i].predict(s0a))\n",
    "    Vs0[i] = np.max(Qs0a)\n",
    "plt.plot(Vs0)\n",
    "\n",
    "# Bellman residual\n",
    "residual = []\n",
    "for i in range(1,nb_iter):\n",
    "    residual.append(np.mean((Qfunctions[i].predict(SA)-Qfunctions[i-1].predict(SA))**2))\n",
    "plt.figure()\n",
    "plt.plot(residual);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7e6552-08e7-4a62-81c4-287a08e6e339",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Evaluate graphically and qualitatively how the last learned policy performs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c0736-45d4-49c5-8230-984f332f9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_action(Q,s,nb_actions):\n",
    "    Qsa = []\n",
    "    for a in range(nb_actions):\n",
    "        sa = np.append(s,a).reshape(1, -1)\n",
    "        Qsa.append(Q.predict(sa))\n",
    "    return np.argmax(Qsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e578ad19-6985-4855-9a56-f2559e97c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "cartpole = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "s,_ = cartpole.reset()\n",
    "for t in range(500):\n",
    "    a = greedy_action(Qfunctions[-1],s,cartpole.action_space.n)\n",
    "    s2,r,d,trunc,_ = cartpole.step(a)\n",
    "    cartpole.render()\n",
    "    s = s2\n",
    "    if d:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e48e7a-c269-41e7-ad3d-85727f041204",
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37abc045-1ed1-4119-a83e-1bf589b1c825",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Eventually, we have a fully offline procedure that seems to achieve an efficient control for the cart-pole (or acrobot) environment. This is a rather strong result: with fully offline samples we seem to be able to learn good controlers.\n",
    "\n",
    "But there are limits to this approach, mostly due to the mismatch between $\\rho$ and the states and actions actually visited by the learned policy $\\pi$.  \n",
    "What would have happened if, in acrobot, there had been no sample with `done=True` in the training set?  \n",
    "Isn't the totally random collection of samples a waste of computational resources in some states, and a lack of exploration in others?\n",
    "\n",
    "Some samples in the training set might be very redundant or completely unrelated to states encountered by $\\pi$. Their presence in the training set biases the loss. Additionally, collecting them did cost us some computing time, which might have been better used.  \n",
    "Conversely, some states and actions visited by $\\pi$ might be absent from the support of $\\rho$. Because of this, there are no good estimates of $q(s,a)$ obtained from the training set, which prevent actually incrementally improving $\\pi$.  \n",
    "We discussed this in the previous chapter when we covered the importance of the behavior distribution.\n",
    "\n",
    "A simple empirical fix to this problem would be to enrich the training set incrementally, by performing rounds of data collection at every iteration of FQI, collecting samples from $(s,a)$ pairs seen by $\\pi_n$ and also exploring new actions is the corresponding states $s$.\n",
    "\n",
    "We won't implement this now and turn to a more online procedure. However, note that offline RL received a lot of attention and analysis in recent years and is still a very active field of research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c087212-c223-4307-aded-4c0aa93694d5",
   "metadata": {},
   "source": [
    "# Neural networks for AVI: deep Q-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeffdac-73ec-4b06-a862-253186226357",
   "metadata": {},
   "source": [
    "## Back to AVI as a series of risk minimization problems\n",
    "\n",
    "Again, let us return to the original formulation of AVI as a sequence of supervised learning problems.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Approximate value iteration as a sequence of risk minimization problems**  \n",
    "$$\\pi_n \\in \\mathcal{G} q_n,$$\n",
    "$$L_n(\\theta) = \\frac{1}{2} \\mathbb{E}_{(s,a) \\sim \\rho}\\left[ \\left( q(s,a;\\theta) - G^{\\pi_n}_1(s,a,q_n) \\right)^2 \\right],$$\n",
    "$$\\theta_{n+1} \\in \\arg\\min_{\\theta} L_n(\\theta),$$\n",
    "$$q_{n+1}(s,a) = q(s,a;\\theta_{n+1}).$$\n",
    "</div>\n",
    "\n",
    "With \n",
    "$$\\pi \\in \\mathcal{G} q, \\Leftrightarrow \\pi(s) \\in \\arg\\max_{\\pi \\in \\Delta_A} \\mathbb{E}_{a\\sim\\pi} \\left[q(s,a)\\right]$$\n",
    "\n",
    "And\n",
    "$$G^\\pi_1(s,a,Q) = R_0 + \\gamma q(S_1, A_1) \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a\\\\ A_1 \\sim \\pi(S_1),\\\\ S_{1}\\sim p(\\cdot|S_0,A_0),\\\\ R_0 = r(S_0,A_0,S_{1}).\\end{array}$$\n",
    "\n",
    "If this risk is differentiable, ie. if $q$ is differentiable with respect to $\\theta$, provided one can draw a mini-batch of independently and identically drawn samples $\\left\\{\\left(s_i,a_i,r_i,s'_i\\right)\\right\\}_{i\\in [1,B]}$ (either by sampling from a larger training set, or directly from the system to control), with $(s,a) \\sim \\rho(\\cdot)$ and $s',a' \\sim p(s' | s,a)\\pi(a'|s')$, then one can derive a stochastic gradient descent learning procedure and iteratively learn $\\theta_{n+1}$ as the limit of the sequence $\\theta_{k+1} \\leftarrow \\theta_{k} + \\alpha_k d_n(\\theta_{k})$ with \n",
    "$$d_n(\\theta) = \\frac{1}{B} \\sum_{i=1}^B \\left[ \\left( r_i + \\gamma q(s_i',\\pi_n(s'_i);\\theta_{n}) - q(s_i,a_i;\\theta) \\right) \\nabla_\\theta q(s_i,a_i;\\theta) \\right].$$\n",
    "\n",
    "This type of SGD optimization is precisely what is done in neural networks optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059fb6b4-a0cf-459e-a6a4-8b26849cb17d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Do you recall what are the differences with Q-learning?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b7f84-2472-426f-a950-ee6762e186a5",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "In Q-learning, there was no training set and the samples were collected on-the-fly.  \n",
    "So $B=1$. \n",
    "But also, two consecutive samples were higly correlated, breaking the iid assumption.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c61654-5ad0-4ef2-8adf-0bf6663326c0",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "\n",
    "To recover the independence assumption between samples, we can, as previously, introduce the mechanism of [*Experience Replay*](https://link.springer.com/article/10.1007/BF00992699) by storing past samples into a *Replay Memory*. When samples a required for a mini-batch gradient update, the samples are collected uniformly from the replay memory, thus mimicking an (almost) independent draw according to $\\rho(\\cdot)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0da512b-2815-4def-88f7-9d3794ee81f4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Design a class for storing and sampling experience samples. Limit the size of this memory (via a FIFO mechanism) to a fixed number of samples (adapt this number to your computer's RAM). Test it by running a random policy for $2\\cdot 10^6$ time steps.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de800d-411c-4dbb-a8b3-d9d055ea9b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/replay_buffer.py\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # capacity of the buffer\n",
    "        self.data = []\n",
    "        self.index = 0 # index of the next cell to be filled\n",
    "    def append(self, s, a, r, s_, d):\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(None)\n",
    "        self.data[self.index] = (s, a, r, s_, d)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.data, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41da5fb-93fa-486c-8e29-d911499b34ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing insertion in the ReplayBuffer class\n",
    "from tqdm import trange\n",
    "import gymnasium as gym\n",
    "cartpole = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "replay_buffer_size = int(1e6)\n",
    "nb_samples = int(2e6)\n",
    "\n",
    "memory = ReplayBuffer(replay_buffer_size)\n",
    "state, _ = cartpole.reset()\n",
    "print(\"Testing insertion of\", nb_samples, \"samples in the replay buffer\")\n",
    "for _ in trange(nb_samples):\n",
    "    action = cartpole.action_space.sample()\n",
    "    next_state, reward, done, trunc, _ = cartpole.step(action)\n",
    "    memory.append(state, action, reward, next_state, done)\n",
    "    if done:\n",
    "        state, _ = cartpole.reset()\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "print(\"Replay buffer size:\", len(memory))\n",
    "\n",
    "# Testing sampling in the ReplayBuffer class\n",
    "nb_batches = int(1e4)\n",
    "batch_size = 50\n",
    "import random\n",
    "\n",
    "print(\"Testing sampling of\", nb_batches, \"minibatches of size\", batch_size, \"from the replay buffer\")\n",
    "for _ in trange(nb_batches):\n",
    "    batch = memory.sample(batch_size)\n",
    "\n",
    "print(\"Example of a 2-sample minibatch\", memory.sample(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e145d-c025-4d8a-8c9c-cc082acde5a3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Know your data structures!**\n",
    "\n",
    "Taking some time to think about what makes a good replay buffer may save you hours of puzzled head-banging. It is important to know what you expect from your replay buffer to choose the right data structure.\n",
    "\n",
    "Common mistake: we need a fixed sized memory, that works like a FIFO structure, so let's pick a [deque](https://docs.python.org/3/library/collections.html#collections.deque) (double-ended queue). That sounds fair, deques (in Python) have a fixed maximum size and constant time insertion at the beginning and the end. BUT they also have [$O(n)$ access time](https://wiki.python.org/moin/TimeComplexity) which means that for large replay buffers, sampling a minibatch may take forever.\n",
    "\n",
    "Here, what we really need is a fixed-size FIFO, with $O(1)$ insertion at the end and $O(1)$ access.\n",
    "\n",
    "This should motivate the choices made in the replay buffer class above.\n",
    "\n",
    "If you want empirical verification of the reasoning above, in the \"replay memory\" folder, you will find a short notebook enabling empirical testing of many different data structures for replay buffers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a88e4b6-d302-4e7b-bcb9-09c27a319515",
   "metadata": {},
   "source": [
    "## A deep Q-network\n",
    "\n",
    "The term Deep Q-Network was coined by the (now historical) paper **[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)** by Mnih et al. (2013) that put forward the main ideas we develop here. All those were later popularized by DeepMind's paper in Nature **[Human-level control through deep reinforcement learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)** by Mnih et al. (2015).\n",
    "\n",
    "Let's design a (deep) neural network that will serve as a function approximator for $q(s,a;\\theta)$. \n",
    "\n",
    "<center><img src=\"img/dqlas.png\" height=\"15%\" width=\"15%\"></img></center>\n",
    "\n",
    "Note that since we're going to have to compute $\\max_a q(s,a)$ and since actions are discrete, it is preferable to avoid running as many passes through the network as there are actions. Therefore, instead of the network structure above, we will prefer to use the one below.\n",
    "\n",
    "<center><img src=\"img/dqls.png\" height=\"30%\" width=\"30%\"></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76cdcd-fd1a-4648-b7fe-ff0aed3015f2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Declare a simple neural network for our $q$ function with 2 hidden layers and 24 neurons on each layer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be55baed-c41a-43c1-8988-4c22838b1516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "cartpole = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "state_dim = cartpole.observation_space.shape[0]\n",
    "n_action = cartpole.action_space.n \n",
    "nb_neurons=24\n",
    "\n",
    "DQN = torch.nn.Sequential(nn.Linear(state_dim, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(nb_neurons, n_action)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d815c8a5-1c51-410a-8f1a-0650b8d24c26",
   "metadata": {},
   "source": [
    "We're almost there. Now we can implement the algorithm that:\n",
    "- takes $\\epsilon$-greedy actions with respect to $q$\n",
    "- stores samples in the replay buffer\n",
    "- at each interaction step with the environment, draws a mini-batch, computes the target values for each $(s,a)$ and takes a gradient step.\n",
    "- repeats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475398b-6d12-4cc4-98c4-ca21937d92d5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Write this algorithm's pseudo-code.  \n",
    "You can take inspiration from the algorithm on page 5 of **[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)**.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb22414-b1a3-4660-8d24-7d211c0dbfb1",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "         state = init()\n",
    "         loop:\n",
    "            action = greedy_action(DQN) or random_action()\n",
    "            new_state, reward = step(state, action)\n",
    "            replay_memory.add(state, action, reward, new_state)\n",
    "            minibatch = replay_memory.sample(minibatch_size)\n",
    "            X_train = Y_train = []\n",
    "            for (s,a,r,s') in minibatch:\n",
    "                Q  = DQN.predict(s)\n",
    "                Q' = DQN.predict(s')\n",
    "                if non-terminal(s'): \n",
    "                    update = r + gamma * max(Q')    \n",
    "                else:  \n",
    "                    update = r\n",
    "                Q[a] = update\n",
    "                X_train.append(s)\n",
    "                Y_train.append(Q)\n",
    "            DQN.train_one_step(X_train,Y_train)\n",
    "            state = new_state\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feecf188-38b8-45d0-b574-478f04aef56d",
   "metadata": {},
   "source": [
    "Now we can proceed with the implementation. This series of exercises break it down into small steps.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "It will actually be useful to have separate torch.Tensor for the each element type in the sampled minibatch.  \n",
    "That is one Tensor for a minibatch of states, another for actions, etc.\n",
    "Let's redefine the sample function of our replay buffer class to that end.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4be28c-7391-438b-ab90-58df9b13f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/replay_buffer2.py\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.capacity = int(capacity) # capacity of the buffer\n",
    "        self.data = []\n",
    "        self.index = 0 # index of the next cell to be filled\n",
    "        self.device = device\n",
    "    def append(self, s, a, r, s_, d):\n",
    "        if len(self.data) < self.capacity:\n",
    "            self.data.append(None)\n",
    "        self.data[self.index] = (s, a, r, s_, d)\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.data, batch_size)\n",
    "        return list(map(lambda x:torch.Tensor(np.array(x)).to(self.device), list(zip(*batch))))\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc31060-8df4-4c5e-a96d-7f2e0163b6f0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Define a utility function that computes the greedy action from a DQN and a batch of states.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0887978e-7aa2-4b51-8a26-74ca701e8c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/dqn_greedy_action.py\n",
    "import torch\n",
    "\n",
    "def greedy_action(network, state):\n",
    "    device = \"cuda\" if next(network.parameters()).is_cuda else \"cpu\"\n",
    "    with torch.no_grad():\n",
    "        Q = network(torch.Tensor(state).unsqueeze(0).to(device))\n",
    "        return torch.argmax(Q).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d0e8b-8bb9-4aec-a539-d02d2884da56",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Write a class that keeps a replay buffer as internal attribute and implements the pseudo-code you wrote earlier.  \n",
    "Here are a few tips:\n",
    "- To ensure exploration, take a constant $\\epsilon_{max}$ value during $\\tau_{delay}$ time steps, then substract $\\epsilon_{step}$ from $\\epsilon$ at every time step until you reach time $\\tau_{period}$.\n",
    "- A common optimizer (instead of plain SGD) is ADAM (or RMSprop).\n",
    "- Assume the interaction with the environment will be episodic. After each training episode store the episode's cumulated return for monitoring.\n",
    "\n",
    "Write your class but don't run this code just yet!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e88c2fd-ccc3-45da-af5a-306d5a476d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from solutions.replay_buffer2 import ReplayBuffer\n",
    "from solutions.dqn_greedy_action import greedy_action\n",
    "\n",
    "class dqn_agent:\n",
    "    def __init__(self, config, model):\n",
    "        device = \"cuda\" if next(model.parameters()).is_cuda else \"cpu\"\n",
    "        self.gamma = config['gamma']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.nb_actions = config['nb_actions']\n",
    "        self.memory = ReplayBuffer(config['buffer_size'], device)\n",
    "        self.epsilon_max = config['epsilon_max']\n",
    "        self.epsilon_min = config['epsilon_min']\n",
    "        self.epsilon_stop = config['epsilon_decay_period']\n",
    "        self.epsilon_delay = config['epsilon_delay_decay']\n",
    "        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop\n",
    "        self.model = model \n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    def gradient_step(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            X, A, R, Y, D = self.memory.sample(self.batch_size)\n",
    "            QYmax = self.model(Y).max(1)[0].detach()\n",
    "            #update = torch.addcmul(R, self.gamma, 1-D, QYmax)\n",
    "            update = torch.addcmul(R, 1-D, QYmax, value=self.gamma)\n",
    "            QXA = self.model(X).gather(1, A.to(torch.long).unsqueeze(1))\n",
    "            loss = self.criterion(QXA, update.unsqueeze(1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step() \n",
    "    \n",
    "    def train(self, env, max_episode):\n",
    "        episode_return = []\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        epsilon = self.epsilon_max\n",
    "        step = 0\n",
    "\n",
    "        while episode < max_episode:\n",
    "            # update epsilon\n",
    "            if step > self.epsilon_delay:\n",
    "                epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n",
    "\n",
    "            # select epsilon-greedy action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(self.model, state)\n",
    "\n",
    "            # step\n",
    "            next_state, reward, done, trunc, _ = env.step(action)\n",
    "            self.memory.append(state, action, reward, next_state, done)\n",
    "            episode_cum_reward += reward\n",
    "\n",
    "            # train\n",
    "            self.gradient_step()\n",
    "\n",
    "            # next transition\n",
    "            step += 1\n",
    "            if done:\n",
    "                episode += 1\n",
    "                print(\"Episode \", '{:3d}'.format(episode), \n",
    "                      \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                      \", batch size \", '{:5d}'.format(len(self.memory)), \n",
    "                      \", episode return \", '{:4.1f}'.format(episode_cum_reward),\n",
    "                      sep='')\n",
    "                state, _ = env.reset()\n",
    "                episode_return.append(episode_cum_reward)\n",
    "                episode_cum_reward = 0\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "        return episode_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf48bd3-2d89-43d2-9bf1-e76096021180",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Train for 200 episodes on CartPole, with a learning rate of $0.001$, abatch size of $20$, $\\gamma=0.95$ and a replay buffer of maximum $1000000$ samples. Take $\\epsilon_{max}=1$, $\\epsilon_{min}=0.01$, $\\tau_{delay}=20$ and $\\tau_{period}=1000$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cd099b-b016-41de-a549-e82c16281148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "cartpole = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Declare network\n",
    "state_dim = cartpole.observation_space.shape[0]\n",
    "n_action = cartpole.action_space.n \n",
    "nb_neurons=24\n",
    "DQN = torch.nn.Sequential(nn.Linear(state_dim, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(nb_neurons, n_action)).to(device)\n",
    "\n",
    "# DQN config\n",
    "config = {'nb_actions': cartpole.action_space.n,\n",
    "          'learning_rate': 0.001,\n",
    "          'gamma': 0.95,\n",
    "          'buffer_size': 1000000,\n",
    "          'epsilon_min': 0.01,\n",
    "          'epsilon_max': 1.,\n",
    "          'epsilon_decay_period': 1000,\n",
    "          'epsilon_delay_decay': 20,\n",
    "          'batch_size': 20}\n",
    "\n",
    "# Train agent\n",
    "agent = dqn_agent(config, DQN)\n",
    "scores = agent.train(cartpole, 200)\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291d6a4-d1f6-476f-87aa-e72c52016657",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Observe how the learned policy behaves.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c25b77-25e2-4064-9605-20b818f7a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from solutions.dqn_greedy_action import greedy_action\n",
    "\n",
    "cartpole = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "s,_ = cartpole.reset()\n",
    "for t in range(1000):\n",
    "    a = greedy_action(DQN,s)\n",
    "    s2,r,d,trunc,_ = cartpole.step(a)\n",
    "    cartpole.render()\n",
    "    s = s2\n",
    "    if d:\n",
    "        break\n",
    "\n",
    "cartpole.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d0235-a418-4098-845b-9aac0fc77478",
   "metadata": {},
   "source": [
    "## Target networks\n",
    "\n",
    "Let's take a step back. In the end, we have done something quite similar to Q-learning: at every iteration of AVI, the $q_{n+1} = \\mathbb{A}\\mathbb{T}^* q_n$ we compute is obtained by taking a single gradient step in the direction of $\\mathbb{T}^* q_n$. This enabled having a single network in memory, and using it to bootstrap its own training samples.\n",
    "\n",
    "A single gradient step makes for a poor function approximator. In turn, the sequence of $q_n$ functions might be quite unstable and noisy, in particular with environments that have large variance in $s'$ and $r$ (which is not the case for the cart-pole environment). This can be greatly improved by taking several gradient steps on a given loss function $L_n$, instead of taking a single step and changing the loss function after each sample. This idea is the same as that of delayed updates that we introduced in the chapter on temporal difference learning. \n",
    "\n",
    "So, instead of having a single network, we wish to keep $q_n$ in memory so that we can refine our approximation $q_{n+1}$.\n",
    "\n",
    "In practice, this is achieved by the introduction of a *target network* whose parameters are noted $\\theta^-$, and who plays the role of $q_n$ when learning $q_{n+1}$. \n",
    "Surprisingly and despite its straightforward roots in AVI, this idea was not implemented in the 2013 **[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)** paper which introduced DQN. One had to wait for the 2015 **[Human-level control through deep reinforcement learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)** paper to see the introduction of target networks. So the loss of DQN with a target network is:\n",
    "$$L(\\theta) = \\mathbb{E}_{s,a,r,s'} \\left[ \\left( r + \\gamma \\max_{a'} q(s',a',\\theta^-) - q(s,a;\\theta) \\right) ^2 \\right],$$\n",
    "and the target network parameters $\\theta^-$ are only updated with the Q-network parameters $\\theta_n$ every $C$ steps and are held fixed between individual updates.\n",
    "\n",
    "Note that more recent approaches smooth out this accumulation process by defining an exponential moving average update at every time step, of the form:\n",
    "$$\\theta^- \\leftarrow \\tau \\theta + (1-\\tau) \\theta^-.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaefe962-bbbe-4b54-ab24-298edc429bf0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Is this the same thing as taking several gradient steps after each sample?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f834926-f623-4610-896b-f399c01b5758",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Beware, here be dragons!   \n",
    "\n",
    "No, this is not the same thing. At all.\n",
    "By synchronizing gradient steps, we make sure the replay buffer always tracks the behavior distribution of the current policy. That is: the replay buffer features samples that correspond to trying greedy or exploratory actions in states which are likely to be encountered, and hence, it contains crucial information for policy improvement.\n",
    "\n",
    "Here, by using target networks, we don't break this synchronization. We retain it as we update $\\theta$. We simply allow $q_\\theta$ to better fit $\\mathbb{T}^* q_n$ before we move on to the next iteration of AVI.\n",
    "\n",
    "Conversely, taking several gradient steps on $L_n$ without collecting new data implies taking the risk that the behavior policy's state distribution drifts away from the replay buffer's empirical distribution. In turn, this leads to the risk that gradient updates after the first one are ill-informed. In very easy environments (eg. when state space coverage is rather easy to achieve, like in cart-pole), this can be benign and go unnoticed. But beware: in many difficult situations (eg. when the environment is difficult to explore, when stochasticity is high, when state distributions change quickly with parameter updates, etc.) this can quickly lead to policies that make no sense and eventually perform really badly.\n",
    "\n",
    "An interesting analysis of this dependency can be found in the **[The Difficulty of Passive Learning in Deep Reinforcement Learning](https://openreview.net/forum?id=nPHA8fGicZk)** paper by Ostrovski et al. (2021).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb546bcc-5a81-4017-8df0-95a6c72e1cc4",
   "metadata": {},
   "source": [
    "## Losses and optimizers\n",
    "\n",
    "It might seem trivial, but the question of which optimizer to choose is quite open in RL. Most common choices are RMSprop or Adam (with, sometimes, carefully chosen hyperparameters). For the example of CartPole, it seems Adam with default parameters works better, but that's not a general rule.\n",
    "\n",
    "Another common pratice to stabilize learning is to clip the value of the loss' gradient between $-1$ and $1$. This is not such an uncommon trick, it actually amounts to using an L2 loss for values of the loss between $-1$ and $1$ and an L1 loss outside of this domain. This is also know as the [Huber Loss](https://en.wikipedia.org/wiki/Huber_loss) or the [smooth L1 loss](https://pytorch.org/docs/stable/nn.html#smoothl1loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df8020-5f4b-4db7-840d-9ce37520ec33",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise:**  \n",
    "Modify the previous `dqn_agent` class to include a target network, with an option to perform periodic updates or moving average updates, the possibility to take several gradient steps after each sample, and options for choosing the loss and the optimizer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a797de01-ffe8-46f0-9bb4-8656fd8e618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/dqn_agent.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from solutions.replay_buffer2 import ReplayBuffer\n",
    "from solutions.dqn_greedy_action import greedy_action\n",
    "\n",
    "class dqn_agent:\n",
    "    def __init__(self, config, model):\n",
    "        device = \"cuda\" if next(model.parameters()).is_cuda else \"cpu\"\n",
    "        self.nb_actions = config['nb_actions']\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.95\n",
    "        self.batch_size = config['batch_size'] if 'batch_size' in config.keys() else 100\n",
    "        buffer_size = config['buffer_size'] if 'buffer_size' in config.keys() else int(1e5)\n",
    "        self.memory = ReplayBuffer(buffer_size,device)\n",
    "        self.epsilon_max = config['epsilon_max'] if 'epsilon_max' in config.keys() else 1.\n",
    "        self.epsilon_min = config['epsilon_min'] if 'epsilon_min' in config.keys() else 0.01\n",
    "        self.epsilon_stop = config['epsilon_decay_period'] if 'epsilon_decay_period' in config.keys() else 1000\n",
    "        self.epsilon_delay = config['epsilon_delay_decay'] if 'epsilon_delay_decay' in config.keys() else 20\n",
    "        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop\n",
    "        self.model = model \n",
    "        self.target_model = deepcopy(self.model).to(device)\n",
    "        self.criterion = config['criterion'] if 'criterion' in config.keys() else torch.nn.MSELoss()\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.optimizer = config['optimizer'] if 'optimizer' in config.keys() else torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.nb_gradient_steps = config['gradient_steps'] if 'gradient_steps' in config.keys() else 1\n",
    "        self.update_target_strategy = config['update_target_strategy'] if 'update_target_strategy' in config.keys() else 'replace'\n",
    "        self.update_target_freq = config['update_target_freq'] if 'update_target_freq' in config.keys() else 20\n",
    "        self.update_target_tau = config['update_target_tau'] if 'update_target_tau' in config.keys() else 0.005\n",
    "    \n",
    "    def gradient_step(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            X, A, R, Y, D = self.memory.sample(self.batch_size)\n",
    "            QYmax = self.target_model(Y).max(1)[0].detach()\n",
    "            update = torch.addcmul(R, 1-D, QYmax, value=self.gamma)\n",
    "            QXA = self.model(X).gather(1, A.to(torch.long).unsqueeze(1))\n",
    "            loss = self.criterion(QXA, update.unsqueeze(1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step() \n",
    "    \n",
    "    def train(self, env, max_episode):\n",
    "        episode_return = []\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        epsilon = self.epsilon_max\n",
    "        step = 0\n",
    "        while episode < max_episode:\n",
    "            # update epsilon\n",
    "            if step > self.epsilon_delay:\n",
    "                epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n",
    "            # select epsilon-greedy action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(self.model, state)\n",
    "            # step\n",
    "            next_state, reward, done, trunc, _ = env.step(action)\n",
    "            self.memory.append(state, action, reward, next_state, done)\n",
    "            episode_cum_reward += reward\n",
    "            # train\n",
    "            for _ in range(self.nb_gradient_steps): \n",
    "                self.gradient_step()\n",
    "            # update target network if needed\n",
    "            if self.update_target_strategy == 'replace':\n",
    "                if step % self.update_target_freq == 0: \n",
    "                    self.target_model.load_state_dict(self.model.state_dict())\n",
    "            if self.update_target_strategy == 'ema':\n",
    "                target_state_dict = self.target_model.state_dict()\n",
    "                model_state_dict = self.model.state_dict()\n",
    "                tau = self.update_target_tau\n",
    "                for key in model_state_dict:\n",
    "                    target_state_dict[key] = tau*model_state_dict[key] + (1-tau)*target_state_dict[key]\n",
    "                target_model.load_state_dict(target_state_dict)\n",
    "            # next transition\n",
    "            step += 1\n",
    "            if done or trunc:\n",
    "                episode += 1\n",
    "                print(\"Episode \", '{:3d}'.format(episode), \n",
    "                      \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                      \", batch size \", '{:5d}'.format(len(self.memory)), \n",
    "                      \", episode return \", '{:4.1f}'.format(episode_cum_reward),\n",
    "                      sep='')\n",
    "                state, _ = env.reset()\n",
    "                episode_return.append(episode_cum_reward)\n",
    "                episode_cum_reward = 0\n",
    "            else:\n",
    "                state = next_state\n",
    "        return episode_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b65f5-1424-40fd-8325-95fd76741acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "cartpole = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Declare network\n",
    "state_dim = cartpole.observation_space.shape[0]\n",
    "n_action = cartpole.action_space.n \n",
    "nb_neurons=24\n",
    "DQN = torch.nn.Sequential(nn.Linear(state_dim, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(nb_neurons, n_action)).to(device)\n",
    "\n",
    "# DQN config\n",
    "config = {'nb_actions': cartpole.action_space.n,\n",
    "          'learning_rate': 0.001,\n",
    "          'gamma': 0.95,\n",
    "          'buffer_size': 1000000,\n",
    "          'epsilon_min': 0.01,\n",
    "          'epsilon_max': 1.,\n",
    "          'epsilon_decay_period': 1000,\n",
    "          'epsilon_delay_decay': 20,\n",
    "          'batch_size': 20,\n",
    "          'gradient_steps': 1,\n",
    "          'update_target_strategy': 'replace', # or 'ema'\n",
    "          'update_target_freq': 50,\n",
    "          'update_target_tau': 0.005,\n",
    "          'criterion': torch.nn.SmoothL1Loss()}\n",
    "\n",
    "# Train agent\n",
    "agent = dqn_agent(config, DQN)\n",
    "scores = agent.train(cartpole, 200)\n",
    "plt.plot(scores);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec63f0-ef83-4ed6-8c5b-158c19aa5173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from solutions.dqn_greedy_action import greedy_action\n",
    "\n",
    "cartpole = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "s,_ = cartpole.reset()\n",
    "for t in range(1000):\n",
    "    a = greedy_action(DQN,s)\n",
    "    s2,r,d,trunc,_ = cartpole.step(a)\n",
    "    cartpole.render()\n",
    "    s = s2\n",
    "    if d:\n",
    "        break\n",
    "\n",
    "cartpole.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba97d82d-3896-4e4f-8f35-ede1dfd66a35",
   "metadata": {},
   "source": [
    "## Monitoring progress\n",
    "\n",
    "In the previous experiments, we have counted the number of steps per training episode. But during these episodes, the applied policy was $\\epsilon$-greedy, not greedy. So this is not an objective measure of performance. Let's try to correct this.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Enhance your previous code with the monitoring, after each episode, of:\n",
    "- the number of steps of the training episode\n",
    "- a Monte Carlo estimate of the total sum of rewards from the (distribution of) starting state,\n",
    "- a Monte Carlo estimate of the sum of discounted rewards from the (distribution of) starting state,\n",
    "- an average, over possible starting states $s_0$ of the current $\\max_a Q(s_0,a)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ab9a0-d468-4fd1-a305-a66a151229f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from solutions.replay_buffer2 import ReplayBuffer\n",
    "from solutions.dqn_greedy_action import greedy_action\n",
    "\n",
    "class dqn_agent:\n",
    "    def __init__(self, config, model):\n",
    "        device = \"cuda\" if next(model.parameters()).is_cuda else \"cpu\"\n",
    "        self.nb_actions = config['nb_actions']\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.95\n",
    "        self.batch_size = config['batch_size'] if 'batch_size' in config.keys() else 100\n",
    "        buffer_size = config['buffer_size'] if 'buffer_size' in config.keys() else int(1e5)\n",
    "        self.memory = ReplayBuffer(buffer_size,device)\n",
    "        self.epsilon_max = config['epsilon_max'] if 'epsilon_max' in config.keys() else 1.\n",
    "        self.epsilon_min = config['epsilon_min'] if 'epsilon_min' in config.keys() else 0.01\n",
    "        self.epsilon_stop = config['epsilon_decay_period'] if 'epsilon_decay_period' in config.keys() else 1000\n",
    "        self.epsilon_delay = config['epsilon_delay_decay'] if 'epsilon_delay_decay' in config.keys() else 20\n",
    "        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop\n",
    "        self.model = model \n",
    "        self.target_model = deepcopy(self.model).to(device)\n",
    "        self.criterion = config['criterion'] if 'criterion' in config.keys() else torch.nn.MSELoss()\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.optimizer = config['optimizer'] if 'optimizer' in config.keys() else torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.nb_gradient_steps = config['gradient_steps'] if 'gradient_steps' in config.keys() else 1\n",
    "        self.update_target_strategy = config['update_target_strategy'] if 'update_target_strategy' in config.keys() else 'replace'\n",
    "        self.update_target_freq = config['update_target_freq'] if 'update_target_freq' in config.keys() else 20\n",
    "        self.update_target_tau = config['update_target_tau'] if 'update_target_tau' in config.keys() else 0.005\n",
    "        self.monitoring_nb_trials = config['monitoring_nb_trials'] if 'monitoring_nb_trials' in config.keys() else 0\n",
    "\n",
    "    def MC_eval(self, env, nb_trials):   # NEW NEW NEW\n",
    "        MC_total_reward = []\n",
    "        MC_discounted_reward = []\n",
    "        for _ in range(nb_trials):\n",
    "            x,_ = env.reset()\n",
    "            done = False\n",
    "            trunc = False\n",
    "            total_reward = 0\n",
    "            discounted_reward = 0\n",
    "            step = 0\n",
    "            while not (done or trunc):\n",
    "                a = greedy_action(self.model, x)\n",
    "                y,r,done,trunc,_ = env.step(a)\n",
    "                x = y\n",
    "                total_reward += r\n",
    "                discounted_reward += self.gamma**step * r\n",
    "                step += 1\n",
    "            MC_total_reward.append(total_reward)\n",
    "            MC_discounted_reward.append(discounted_reward)\n",
    "        return np.mean(MC_discounted_reward), np.mean(MC_total_reward)\n",
    "    \n",
    "    def V_initial_state(self, env, nb_trials):   # NEW NEW NEW\n",
    "        with torch.no_grad():\n",
    "            for _ in range(nb_trials):\n",
    "                val = []\n",
    "                x,_ = env.reset()\n",
    "                val.append(self.model(torch.Tensor(x).unsqueeze(0).to(device)).max().item())\n",
    "        return np.mean(val)\n",
    "    \n",
    "    def gradient_step(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            X, A, R, Y, D = self.memory.sample(self.batch_size)\n",
    "            QYmax = self.target_model(Y).max(1)[0].detach()\n",
    "            update = torch.addcmul(R, 1-D, QYmax, value=self.gamma)\n",
    "            QXA = self.model(X).gather(1, A.to(torch.long).unsqueeze(1))\n",
    "            loss = self.criterion(QXA, update.unsqueeze(1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step() \n",
    "    \n",
    "    def train(self, env, max_episode):\n",
    "        episode_return = []\n",
    "        MC_avg_total_reward = []   # NEW NEW NEW\n",
    "        MC_avg_discounted_reward = []   # NEW NEW NEW\n",
    "        V_init_state = []   # NEW NEW NEW\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        epsilon = self.epsilon_max\n",
    "        step = 0\n",
    "        while episode < max_episode:\n",
    "            # update epsilon\n",
    "            if step > self.epsilon_delay:\n",
    "                epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n",
    "            # select epsilon-greedy action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(self.model, state)\n",
    "            # step\n",
    "            next_state, reward, done, trunc, _ = env.step(action)\n",
    "            self.memory.append(state, action, reward, next_state, done)\n",
    "            episode_cum_reward += reward\n",
    "            # train\n",
    "            for _ in range(self.nb_gradient_steps): \n",
    "                self.gradient_step()\n",
    "            # update target network if needed\n",
    "            if self.update_target_strategy == 'replace':\n",
    "                if step % self.update_target_freq == 0: \n",
    "                    self.target_model.load_state_dict(self.model.state_dict())\n",
    "            if self.update_target_strategy == 'ema':\n",
    "                target_state_dict = self.target_model.state_dict()\n",
    "                model_state_dict = self.model.state_dict()\n",
    "                tau = self.update_target_tau\n",
    "                for key in model_state_dict:\n",
    "                    target_state_dict[key] = tau*model_state_dict + (1-tau)*target_state_dict\n",
    "                target_model.load_state_dict(target_state_dict)\n",
    "            # next transition\n",
    "            step += 1\n",
    "            if done or trunc:\n",
    "                episode += 1\n",
    "                # Monitoring\n",
    "                if self.monitoring_nb_trials>0:\n",
    "                    MC_dr, MC_tr = self.MC_eval(env, self.monitoring_nb_trials)    # NEW NEW NEW\n",
    "                    V0 = self.V_initial_state(env, self.monitoring_nb_trials)   # NEW NEW NEW\n",
    "                    MC_avg_total_reward.append(MC_tr)   # NEW NEW NEW\n",
    "                    MC_avg_discounted_reward.append(MC_dr)   # NEW NEW NEW\n",
    "                    V_init_state.append(V0)   # NEW NEW NEW\n",
    "                    episode_return.append(episode_cum_reward)   # NEW NEW NEW\n",
    "                    print(\"Episode \", '{:2d}'.format(episode), \n",
    "                          \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                          \", batch size \", '{:4d}'.format(len(self.memory)), \n",
    "                          \", ep return \", '{:4.1f}'.format(episode_cum_reward), \n",
    "                          \", MC tot \", '{:6.2f}'.format(MC_tr),\n",
    "                          \", MC disc \", '{:6.2f}'.format(MC_dr),\n",
    "                          \", V0 \", '{:6.2f}'.format(V0),\n",
    "                          sep='')\n",
    "                else:\n",
    "                    episode_return.append(episode_cum_reward)\n",
    "                    print(\"Episode \", '{:2d}'.format(episode), \n",
    "                          \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                          \", batch size \", '{:4d}'.format(len(self.memory)), \n",
    "                          \", ep return \", '{:4.1f}'.format(episode_cum_reward), \n",
    "                          sep='')\n",
    "\n",
    "                \n",
    "                state, _ = env.reset()\n",
    "                episode_cum_reward = 0\n",
    "            else:\n",
    "                state = next_state\n",
    "        return episode_return, MC_avg_discounted_reward, MC_avg_total_reward, V_init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245188ab-f471-4715-9cab-e4397316f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "cartpole = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Declare network\n",
    "state_dim = cartpole.observation_space.shape[0]\n",
    "n_action = cartpole.action_space.n \n",
    "nb_neurons=24\n",
    "DQN = torch.nn.Sequential(nn.Linear(state_dim, nb_neurons),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(nb_neurons, nb_neurons),\n",
    "                          nn.ReLU(), \n",
    "                          nn.Linear(nb_neurons, n_action)).to(device)\n",
    "\n",
    "# DQN config\n",
    "config = {'nb_actions': cartpole.action_space.n,\n",
    "          'learning_rate': 0.001,\n",
    "          'gamma': 0.95,\n",
    "          'buffer_size': 1000000,\n",
    "          'epsilon_min': 0.01,\n",
    "          'epsilon_max': 1.,\n",
    "          'epsilon_decay_period': 1000,\n",
    "          'epsilon_delay_decay': 20,\n",
    "          'batch_size': 20,\n",
    "          'gradient_steps': 1,\n",
    "          'update_target_strategy': 'replace', # or 'ema'\n",
    "          'update_target_freq': 50,\n",
    "          'update_target_tau': 0.005,\n",
    "          'criterion': torch.nn.SmoothL1Loss(),\n",
    "          'monitoring_nb_trials': 50}\n",
    "\n",
    "# Train agent\n",
    "agent = dqn_agent(config, DQN)\n",
    "ep_length, disc_rewards, tot_rewards, V0 = agent.train(cartpole, 200)\n",
    "plt.plot(ep_length, label=\"training episode length\")\n",
    "plt.plot(tot_rewards, label=\"MC eval of total reward\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(disc_rewards, label=\"MC eval of discounted reward\")\n",
    "plt.plot(V0, label=\"average $max_a Q(s_0)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ddf33-dfdd-417f-9907-9cde397409f7",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Have you noted that $q$ overestimates the true value of the optimal policy? More on this in [this paper](https://papers.nips.cc/paper/3964-double-q-learning) if you are interested.  \n",
    "Have you noted that the greedy policy is optimal much sooner than the $q$ function?  \n",
    "Have you remarked that the greedy policy performs well much sooner than the agent's policy (which is $\\epsilon$-greedy)?  \n",
    "Can you anticipate the importance of well tuned exploration, for instance if the state space is much larger, or if parts of it are very hard to reach?  \n",
    "Can you anticipate the effect of taking a larger $\\gamma$?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917d67e-5ae6-4be7-8c01-ca857956d533",
   "metadata": {},
   "source": [
    "## DQN on image-based tasks\n",
    "\n",
    "Now it's time to turn towards Pong. As you noted earlier, the frame information in Pong is not sufficient to define an MDP, but stacking several frames together allows to recover the Markov property.\n",
    "\n",
    "We could wish to modify the previous replay buffer so that frames are stored only once (for memory efficiency). Then this new replay buffer would still need to return stacks of 4 frames when `sample()` is called.\n",
    "\n",
    "Fortunately, there's a simpler way to do that.\n",
    "We can use another wrapper so that calling env.step(a) returns a stack of 4 frames.\n",
    "This wrapper actually only stores each frame once which optimizes memory efficiency. This way, we can keep on using our previous replay buffer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cbc6e8-3c76-42e8-a7e8-8a33d71a2e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack\n",
    "pong = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "pong = AtariPreprocessing(pong)\n",
    "pong = FrameStack(pong, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b12eb36-127a-4075-9f1b-68a4a40b87ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,_ = pong.reset()\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe5726-663e-40fc-933d-d3d1e318ec31",
   "metadata": {},
   "source": [
    "The two DQN papers ([Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602) and [Human-level control through deep reinforcement learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)) actually introduce two different neural network architectures.\n",
    "\n",
    "The 2013 paper uses this architecture:\n",
    "- input: $84\\times 84\\times 4$ image (the last 4 frames)\n",
    "- layer 1: Convolutions with 16 filters of size $8\\times 8$ and stride 4. The activation is a ReLU function.\n",
    "- layer 2: Convolutions with 32 filters of size $4\\times 4$ and stride 2. The activation is a ReLU function.\n",
    "- layer 3: Fully connected with 256 ReLU units\n",
    "- layer 4 (output): Fully connected with 2 linear units (one for each action's value)\n",
    "\n",
    "The 2015 paper \n",
    "- input: $84\\times 84\\times 4$ image (the last 4 frames)\n",
    "- layer 1: Convolutions with 32 filters of size $8\\times 8$ and stride 4. The activation is a ReLU function.\n",
    "- layer 2: Convolutions with 64 filters of size $4\\times 4$ and stride 2. The activation is a ReLU function.\n",
    "- layer 3: Convolutions with 64 filters of size $3\\times 3$ and stride 1. The activation is a ReLU function.\n",
    "- layer 4: Fully connected with 512 ReLU units\n",
    "- layer 5 (output): Fully connected with 2 linear units (one for each action's value)\n",
    "\n",
    "Also, it is a good practice to pre-fill the replay buffer with randomly sampled experience. The 2015 paper runs a random policy for 50000 steps to feed the replay buffer before training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed33fc4-af42-4c0c-978b-26aba066fd84",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:** Create the corresponding neural network and adapt your optimization code from the previous exercice to train on Pong (you can take $C$ much larger, in the order of $10000$).\n",
    "</div>\n",
    "\n",
    "Caveat: unless you have a good GPU and a fair amount of time ahead of you (several hours or more), it is recommended to run this computation for a limited number of episodes, on a cloud computing service (or on a dedicated machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425aae5-fd7f-4fbf-a116-5f75375797ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AtariCNN(nn.Module):\n",
    "    def __init__(self, in_channels=4, n_actions=6):\n",
    "        super(AtariCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.head = nn.Linear(512, n_actions)\n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de3768-ba88-4c8b-92a0-1084ef1f43da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack\n",
    "pong = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "pong = AtariPreprocessing(pong)\n",
    "pong = FrameStack(pong,4)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Declare network\n",
    "AtariDQN = AtariCNN()\n",
    "\n",
    "# DQN config\n",
    "config = {'nb_actions': pong.action_space.n,\n",
    "          'learning_rate': 0.001,\n",
    "          'gamma': 0.95,\n",
    "          'buffer_size': 1000000,\n",
    "          'epsilon_min': 0.1,\n",
    "          'epsilon_max': 1.,\n",
    "          'epsilon_decay_period': 1000000,\n",
    "          'epsilon_delay_decay': 0,\n",
    "          'batch_size': 64,\n",
    "          'gradient_steps': 1,\n",
    "          'update_target_strategy': 'replace', # or 'ema'\n",
    "          'update_target_freq': 1000,\n",
    "          'update_target_tau': 0.005,\n",
    "          'criterion': torch.nn.SmoothL1Loss(),\n",
    "          'monitoring_nb_trials': 0}\n",
    "\n",
    "# Declare agent\n",
    "agent = dqn_agent(config, AtariDQN)\n",
    "\n",
    "# pre-fill the replay buffer\n",
    "x,_ = pong.reset()\n",
    "for t in trange(50000):\n",
    "    a = pong.action_space.sample()\n",
    "    y, r, d, tr, _ = pong.step(a)\n",
    "    agent.memory.append(x, a, r, y, d)\n",
    "    if d:\n",
    "        x,_ = pong.reset()\n",
    "    else:\n",
    "        x = y\n",
    "\n",
    "# Train agent\n",
    "ep_length, disc_rewards, tot_rewards, V0 = agent.train(pong, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24745b9-10f9-4bf2-881b-91693c7503d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from solutions.dqn_greedy_action import greedy_action\n",
    "from tqdm import tqdm\n",
    "\n",
    "nb_games=20\n",
    "scores=[]\n",
    "for g in tqdm(range(nb_games)):\n",
    "    s,_ = pong.reset()\n",
    "    score=0\n",
    "    for t in range(1000):\n",
    "        a = greedy_action(AtariDQN,s)\n",
    "        s2,r,d,trunc,_ = pong.step(a)\n",
    "        s = s2\n",
    "        score+=r\n",
    "        if d:\n",
    "            scores.append(score)\n",
    "            break\n",
    "print(\"average score across\", nb_games, \"games:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e82e8e4-8c4f-4065-9d4a-e688b87366ef",
   "metadata": {},
   "source": [
    "To give you an idea of the behavior of a trained agent, you can check the following videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9543ae7d-2e65-4553-96b7-69cf744dd003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"p88R2_3yWPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc512045-07bb-44d1-98b6-c6bb5c20129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "YouTubeVideo(\"TmPfTpjtdgg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5671837d-85ff-4a7a-b4f2-6efb36a442f2",
   "metadata": {},
   "source": [
    "**Going further**\n",
    "\n",
    "A lot of contributions have built on the initial success of DQN. Among those, some are combined and discussed in the **[Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)** paper. We will simply summarize their key ideas here, by decreasing order of importance (according to the paper).\n",
    "\n",
    "- N-step returns. Use samples that accumulate several returns rather than the 1-step return of TD(0).\n",
    "- [Prioritized experience replay](https://arxiv.org/abs/1511.05952) (PER) Inspired by the model-based [Prioritized Sweeping](https://link.springer.com/article/10.1007/BF00993104) approach, PER biases the distribution used to sample mini-batches in order to present high residual samples to the optimizer. Prioritized Sweeping is designed to accelerate the convergence in $L_\\infty$ norm for tabular representations. PER recently received theoretical justification through a variant called [large batch experience replay](https://proceedings.mlr.press/v162/lahire22a/lahire22a.pdf).\n",
    "- [Distributional value functions](https://arxiv.org/abs/1707.06887). Instead of estimating directly $\\mathbb{E}(\\sum_t \\gamma^t r_t)$, first estimate the distribution of $\\sum_t \\gamma^r r_t$, then take the expectation, and iterate. [Three](http://proceedings.mlr.press/v70/bellemare17a.html?trk=public_post_comment-text) [main](https://ojs.aaai.org/index.php/AAAI/article/view/11791) [papers](http://proceedings.mlr.press/v80/dabney18a.html) on the topic and abundant related work (and a [specific book](https://www.distributional-rl.org/)).\n",
    "- [NoisyNet](https://arxiv.org/abs/1706.10295). Instead of an $\\epsilon$-greedy exploration strategy, introduce noise in the network's parameters to drive the exploration.\n",
    "- [Double Q-learning](https://arxiv.org/abs/1509.06461). Q-learning is prone to over-estimation of the true optimal Q function (especially in high variance environments). Double Q-learning aims at compensating this weakness by introducing an under-estimation mechanism based on a second Q function.\n",
    "- [Dueling architecture](https://arxiv.org/abs/1511.06581). The neural network's architecture splits $q$ into the estimation of a value $v(s)$ and an advantage $A(s,a)$ with shared first layers.\n",
    "\n",
    "Beyond these improvements, new work is published each year that leads to a better understanding of the interplay between Deep Learning and RL (eg. the importance of the buffer's distribution, the interpretability of policies, the representations learned, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c7046-6f93-453a-98c2-0746b7d2a257",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this chapter, we have built upon approximate value iteration to build algorithms for large scale problems. For this, we turned to offline regression techniques like random forests, and online ones, like (deep) neural networks.\n",
    "\n",
    "We have seen fitted Q-iteration, its efficiency and limits when the sampling distribution is critical.\n",
    "\n",
    "We have seen Deep Q-networks, their many improvements, their versatility, and the fine tuning of their parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf81ffa-98ba-4403-95a2-72a83bcd1638",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "The homework is mostly a reading (and implementing) list.\n",
    "\n",
    "- [Double DQN](https://ojs.aaai.org/index.php/AAAI/article/view/10295)\n",
    "- n-step returns  \n",
    "- [PER](https://arxiv.org/abs/1511.05952) (maybe [LaBER](https://proceedings.mlr.press/v162/lahire22a/lahire22a.pdf))  \n",
    "- distributional DQN ([C51](http://proceedings.mlr.press/v70/bellemare17a.html?trk=public_post_comment-text))\n",
    "- [NoisyNet](https://arxiv.org/abs/1706.10295) (maybe [NGU](https://arxiv.org/abs/2002.06038))\n",
    "- [Munchausen DQN](https://proceedings.neurips.cc/paper/2020/hash/2c6a0bae0f071cbbf0bb3d5b11d90a82-Abstract.html)\n",
    "- [IMPALA](http://proceedings.mlr.press/v80/espeholt18a.html)\n",
    "\n",
    "A recent promising work: [Beyond the Rainbow](https://arxiv.org/abs/2411.03820v1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a417a26-7b8d-45e6-87ae-be42cab921d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

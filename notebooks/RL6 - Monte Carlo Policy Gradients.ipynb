{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2c7eac-fbff-48e4-abd3-f0fbc8aee546",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://erachelson.github.io/RLclass_MVA/\">https://erachelson.github.io/RLclass_MVA/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887122ef-64e7-42f7-b83e-a684377dede3",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Chapter 6: Monte Carlo policy gradient algorithms</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf06b7-7e7f-4587-ae8d-656436cc4c15",
   "metadata": {},
   "source": [
    "In this chapter we first depart from the (approximate) dynamic programming framework we have worked with so far. We turn back to a criterion for optimality we introduced in the first chapters, namely the average value of a policy across (starting) states. We explore the question of writing the gradient of this criterion with respect to policy parameters, which leads us to a first formulation of this policy gradient a family of associated algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d20f8-d89f-456e-980a-b1a9f2653a42",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Learning outcomes**   \n",
    "By the end of this chapter, you should be able to:\n",
    "- recall and explain the policy gradient theorem for infinite horizon problems,\n",
    "- explain the problem of gradient estimate variance and the introduction of baselines,\n",
    "- implement a REINFORCE algorithm for discrete action spaces,\n",
    "- implement an A2C algorithm for discrete action spaces.\n",
    "\n",
    "Additionally, after doing the homework, you should be able to:\n",
    "- use vectorized environments to accelerate sample collection in policy gradient algorithms,\n",
    "- implement REINFORCE and A2C for continuous action spaces,\n",
    "- derive the policy gradient theorem for finite horizon problems,\n",
    "- explain and implement a generalized advantage estimation method within a policy gradient algorithm,\n",
    "- derive the formulation of off-policy policy gradients,\n",
    "- implement natural policy gradients, TRPO and PPO,\n",
    "- explain and implement the \"OpenAI evolution strategy\" and the \"Canonical evolution strategy\" for gradient-free direct policy search.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150c60fd-7697-4aee-9e2d-db8b90616e2c",
   "metadata": {},
   "source": [
    "# A Monte Carlo policy gradient estimator\n",
    "\n",
    "Recall the policy optimization objective defined in previous chapters, given a distribution $p_0$ on states:  \n",
    "$$J(\\pi) = \\mathbb{E}_{s \\sim p_0} \\left[ V^{\\pi} (s) \\right].$$\n",
    "Or equivalently:  \n",
    "$$J(\\pi) = \\mathbb{E}_{(s_i,a_i)_{i \\in \\mathbb{N}}} \\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)  | \\pi, p_0 \\right].$$\n",
    "\n",
    "We will write $\\tau = (s_t,a_t)_{t\\in \\mathbb{N}}$ and thus $J(\\pi) = \\mathbb{E}_{\\tau} \\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)  | \\pi, p_0 \\right]$.\n",
    "\n",
    "One could imagine a *finite differences* approach to estimate $\\nabla_\\theta J(\\theta)$, but this would require trying out a series of increments $\\Delta \\theta$ which quickly becomes impractical (because the increment size is hard to tune, especially in stochastic systems, and also because of the sample inefficiency of the approach).\n",
    "\n",
    "Remark:\n",
    "- Let's not discard finite difference methods too quickly. They have their merits and showed great successes through methods such as [PEGASUS (Ng and Jordan, 2000)](https://arxiv.org/abs/1301.3878). Also, having random $\\Delta \\theta$ drawn from a Gaussian distribution is essentially what Evolution Strategies do, and [Salimans et al. (2017)](https://arxiv.org/abs/1703.03864) or [Chrabaszcz et al. (2018)](https://arxiv.org/abs/1802.08842) illustrated how that could be a scalable method to obtain gradient estimates in RL. We won't cover these topics here and leave them as exercises.  \n",
    "\n",
    "The key result of this chapter is that one can express the gradient of $J(\\theta)$ as directly proportional to the value of a trajectory and the gradient of $\\pi$:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Monte Carlo policy gradient**  \n",
    "Given trajectories $\\tau=(s_t,a_t)_{t\\in [0,\\infty]}$ drawn according to policy $\\pi$, \n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau} \\left[ \\sum_{t=0}^\\infty \\gamma^t G_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right],$$\n",
    "where $G_t = G^\\pi(s_t,a_t) = \\sum_{t'\\geq t} \\gamma^{t'-t} r_{t'}$ is the return random variable from the state-action pair visited at time step $t$.\n",
    "</div>\n",
    "\n",
    "The interpretation of this estimator is very straightforward:  \n",
    "To increase the average value of policy $\\pi_\\theta$ over the starting distribution $p_0$, we should change $\\theta$ in a direction that is a linear combination of the $\\nabla_\\theta \\log \\pi(a|s)$, where the coefficients are the expected outcomes $G^\\pi(s,a)$ of picking action $a$ in $s$.  \n",
    "Since $\\nabla_\\theta \\log \\pi(a|s)$ is a direction that increases the log probability of $a$ in $s$, we can rephrase the last sentence. The policy gradient tells us:  \n",
    "**To increase the value of the current policy, we should increase the log-probability of $a$ in $s$ in proportion to the expected outcome of $a$ in $s$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed3fb7-e06c-48f4-afbc-717c92935251",
   "metadata": {},
   "source": [
    "Let us derive this result quite naturally.\n",
    "\n",
    "Let us consider trajectories $\\tau = (s_0,a_0,r_0,...)$ drawn according to $\\pi$ from the starting state. Each of these trajectories has an overall payoff of $G(\\tau) = \\sum_t \\gamma^t r_t$, and is drawn with probability density $p(\\tau|\\theta)$. Then the objective function can be written:\n",
    "\\begin{align}\n",
    "J(\\theta) &= \\mathbb{E}_\\tau \\left[ G(\\tau) | \\theta \\right]\\\\\n",
    " &= \\int G(\\tau) p(\\tau | \\theta) d\\tau\n",
    "\\end{align}\n",
    "\n",
    "So the objective function's gradient is:\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta) &= \\nabla_\\theta \\int G(\\tau) p(\\tau|\\theta) d\\tau,\\\\\n",
    " &= \\int G(\\tau) \\nabla_\\theta p(\\tau|\\theta) d\\tau,\\\\\n",
    " &= \\int G(\\tau) p(\\tau|\\theta) \\frac{\\nabla_\\theta p(\\tau|\\theta)}{p(\\tau|\\theta)} d\\tau,\\\\\n",
    " &= \\mathbb{E}_\\tau \\left[ G(\\tau) \\nabla_\\theta \\log p(\\tau|\\theta) \\right].\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992e3d5-bc45-4830-b54a-0463d38fae9d",
   "metadata": {},
   "source": [
    "We have used the fact that $\\nabla_\\theta p(\\tau|\\theta) = p(\\tau|\\theta) \\nabla_\\theta \\log p(\\tau|\\theta)$, sometimes known as the *nabla-log* trick.  \n",
    "Let us study the $\\nabla_\\theta \\log p(\\tau|\\theta)$, term along a series of remarks.\n",
    "\n",
    "**Remark 1: law of $s_{t+1},a_{t+1}$ given the policy and history.**  \n",
    "One has $p(s_{t+1},a_{t+1} | (s_i,a_i)_{i \\in [0,t]}, \\theta) = p(s_{t+1} | (s_i,a_i)_{i \\in [0,t]}, \\theta) p(a_{t+1} | s_{t+1}, (s_i,a_i)_{i \\in [0,t]}, \\theta)$.  \n",
    "But the transition model is Markovian, so $p(s_{t+1} | (s_i,a_i)_{i \\in [0,t]}, \\theta) = p(s_{t+1} | s_t, a_t)$.  \n",
    "And the law of $a_{t+1}$ is given by the policy, so $p(a_{t+1} | s_{t+1}, (s_i,a_i)_{i \\in [0,t]}, \\theta) = \\pi_\\theta(a_{t+1}|s_{t+1})$.  \n",
    "Consequently:\n",
    "$$p(s_{t+1},a_{t+1} | (s_i,a_i)_{i \\in [0,t]}, \\theta) = p(s_{t+1} | s_t, a_t) \\pi_\\theta(a_{t+1}|s_{t+1}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fd6aeb-d089-4a3d-9137-1bcc62fed008",
   "metadata": {},
   "source": [
    "**Remark 2: probability density of a trajectory.**  \n",
    "Recall that $p(\\tau|\\theta) = p((s_t,a_t)_{t\\in [0,\\infty]}|\\theta)$.  \n",
    "This joint probability can be decomposed into conditional probabilities: $$p(\\tau|\\theta) = p(s_0,a_0|\\theta) \\prod_{t=0}^\\infty p(s_{t+1},a_{t+1} | (s_i,a_i)_{i \\in [0,t]}, \\theta).$$ \n",
    "The previous remarks allows us to simplify to: $$p(\\tau|\\theta) = p(s_0,a_0|\\theta) \\prod_{t=0}^\\infty p(s_{t+1} | s_t, a_t) \\pi_\\theta(a_{t+1}|s_{t+1}).$$ \n",
    "By expanding the first term into $p(s_0)\\pi_\\theta(a_0|s_0)$ and reordering the terms inside the product, we obtain:\n",
    "$$p(\\tau|\\theta) = p(s_0) \\prod_{t=0}^\\infty p(s_{t+1} | s_t, a_t) \\pi_\\theta(a_t|s_t).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b526e-e059-4503-9fab-fd7d4654d00e",
   "metadata": {},
   "source": [
    "**Remark 3: grad-log-prob of a trajectory.**  \n",
    "Now let us consider the full $\\nabla_\\theta \\log p(\\tau|\\theta)$ term. The previous remarks tell us that  \n",
    "$$\\nabla_\\theta \\log p(\\tau|\\theta) = \\nabla_\\theta \\log p(s_0) + \\sum_{t=0}^\\infty \\left[ \\nabla_\\theta \\log p(s_{t+1} | s_t, a_t) + \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\right].$$\n",
    "But the initial state distribution and the transition model do not depend on $\\theta$, so this expression boils down to:\n",
    "$$\\nabla_\\theta \\log p(\\tau|\\theta) = \\sum_{t=0}^\\infty \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae851aff-3088-4e95-bded-eae7668ce2df",
   "metadata": {},
   "source": [
    "So eventually, we have a first version of a policy gradient estimator:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Monte Carlo policy gradient (first naive version)**  \n",
    "Given trajectories $\\tau=(s_t,a_t)_{t\\in [0,\\infty]}$ drawn according to policy $\\pi$, and evaluated through a criterion $G(\\tau)$, \n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau} \\left[ G(\\tau) \\sum_{t=0}^\\infty \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right].$$\n",
    "</div>\n",
    "\n",
    "This estimator provides a straightforward algorithm. We could play policy $\\pi_\\theta$ from $p_0$, and collect pairs of states and actions $(s,a)$ along trajectories $\\tau$. After each trajectory terminates, we sum the rewards obtained to get $G(\\tau)$, then iterate over all encountered states and actions to sum the $G(\\tau) \\nabla_\\theta \\log \\pi_\\theta(a|s)$ terms. This yields an ascent direction: we can take a stochastic gradient ascent step, and repeat.\n",
    "\n",
    "This estimator is called **Monte Carlo** because it relies on a (Monte Carlo) simulation of $\\pi$ to obtain realizations of $G(\\tau)$, and uses them as weights of $\\nabla \\log \\pi(a|s)$ in each encountered state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68429d0b-35fe-479e-aef1-e601e5ac4ae8",
   "metadata": {},
   "source": [
    "**Interpretation**  \n",
    "Let us consider a fixed $s$.  \n",
    "Then $\\nabla_\\theta \\log \\pi_\\theta(a|s)$ is the answer to \"in what direction should I change $\\theta$ to increase the log probability of taking action $a$ in $s$?\".  \n",
    "The expression above tells us that in order to improve the value of $\\pi_\\theta$, we should change $\\theta$ in a direction that is a linear combination of all $\\nabla_\\theta \\pi_\\theta(a|s)$, giving more weight to action $a$ in state $s$ in proportion to the value $G(\\tau)$ of the trajectory they belong too.   \n",
    "In even simpler words: if a trajectory was better than another, its action probabilities should be reinforced (that's not the reason for the name of the algorithm, but it could have been).\n",
    "\n",
    "But we can do a bit better than this estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4057df39-f866-4e32-837f-869973ab65c5",
   "metadata": {},
   "source": [
    "**Remark 4: the expected grad-log-prob lemma.**  \n",
    "Let us take a step aside and consider the expectation of $\\nabla_\\theta \\log \\pi_\\theta(a|s)$.  \n",
    "In a given $s$:\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{a\\sim \\pi_\\theta(s)} [ \\nabla_\\theta \\log \\pi_\\theta(a|s) ] &= \\int_A \\pi_\\theta(a|s) \\nabla_\\theta \\log \\pi_\\theta(a|s) da\\\\\n",
    " &= \\int_A \\nabla_\\theta \\pi_\\theta(a|s) da\\\\\n",
    " &= \\nabla_\\theta \\int_A \\pi_\\theta(a|s) da\\\\\n",
    " &= \\nabla_\\theta 1\\\\\n",
    " &= 0\n",
    "\\end{align}\n",
    "So we get that: \n",
    "$$\\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\right] = \\mathbb{E}_{s\\sim\\rho^\\pi} \\left[ \\mathbb{E}_{a\\sim \\pi} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\right] \\right] = \\mathbb{E}_{s\\sim\\rho^\\pi} [0]=  0.$$\n",
    "Actually, we can generalize this immediately: we can multiply $\\nabla_\\theta \\log \\pi_\\theta(a|s)$ by anything that does not depend on $a$ and obtain the same result:\n",
    "$$\\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ b(s)\\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\right] \\right] = 0.$$\n",
    "This will come in handy in a future section about baselines in policy gradients and will serve as a basis for the exercise on [Generalized Advantage Estimation (Schulman et al., 2016)](https://arxiv.org/abs/1506.02438), but for now it will help us finish improving our policy gradient estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf48d4c-29e0-43f2-8bd1-94048f0e7613",
   "metadata": {},
   "source": [
    "**Back to the policy gradient estimator.**  \n",
    "Let us replace $G(\\tau)$ by its expression in terms of $s_t$ and $a_t$:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s_i,a_i)_{i\\in \\mathbb{N}}} \\left[ \\left(\\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t) \\right) \\left( \\sum_{t=0}^\\infty \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right) \\right] .$$\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s_i,a_i)_{i\\in \\mathbb{N}}} \\left[ \\sum_{t=0}^\\infty \\left(\\sum_{t'=0}^\\infty \\gamma^{t'} r(s_{t'},a_{t'}) \\right) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right] .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eee6c94-c30e-4bb3-b789-f7849226bb78",
   "metadata": {},
   "source": [
    "Let us look at one of the terms under the $\\sum_{t=0}^\\infty$ sum: $\\left( \\sum_{t'=0}^\\infty \\gamma^{t'} r(s_{t'},a_{t'}) \\right) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$.  \n",
    "It seems awkward that $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$ be weighted by rewards that were obtained *before* $t$.  \n",
    "So let us cut the $\\sum_{t'=0}^\\infty$ in two; the rewards obtained up to $t-1$ and those obtained after $t$:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s_i,a_i)_{i\\in \\mathbb{N}}} \\left[ \\sum_{t=0}^\\infty \\left(\\sum_{t'=0}^{t-1} \\gamma^{t'} r(s_{t'},a_{t'}) + \\sum_{t'=t}^\\infty \\gamma^{t'} r(s_{t'},a_{t'}) \\right) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right] .$$\n",
    "\n",
    "Let us develop this sum:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s_i,a_i)_{i\\in \\mathbb{N}}} \\left[ \\sum_{t=0}^\\infty \\left(\\sum_{t'=0}^{t-1} \\gamma^{t'} r(s_{t'},a_{t'})\\right) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) + \\sum_{t=0}^\\infty \\left(\\sum_{t'=t}^\\infty \\gamma^{t'} r(s_{t'},a_{t'}) \\right) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right] .$$\n",
    "\n",
    "When we look at the first term in the sum, we remark that $\\sum_{t'=0}^{t-1} \\gamma^{t'} r(s_{t'},a_{t'})$ does not depend on $s_t,a_t$. These are rewards that were obtained in the past, before $t$. By application of the expected grad-log-prob lemma, we have immediately:\n",
    "$$\\mathbb{E}_{(s_i,a_i)_{i\\in \\mathbb{N}}} \\left[ \\sum_{t=0}^\\infty \\left(\\sum_{t'=0}^{t-1} \\gamma^{t'} r(s_{t'},a_{t'})\\right) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right] = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08788563-fec1-4374-a606-0d738f7098d7",
   "metadata": {},
   "source": [
    "<details class=\"alert \">\n",
    "    <summary markdown=\"span\"><b>If you are not convinced and want to see the calculation in more detail, click this box to expand it.</b></summary>\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{(s_i,a_i)_{i\\in \\mathbb{N}}} \\left[ \\sum_{t=0}^\\infty \\left(\\sum_{t'=0}^{t-1} \\gamma^{t'} r(s_{t'},a_{t'})\\right) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right] &= \\sum_{t=0}^\\infty \\mathbb{E}_{(s_i,a_i)_{i\\in \\mathbb{N}}} \\left[ \\left(\\sum_{t'=0}^{t-1} \\gamma^{t'} r(s_{t'},a_{t'})\\right) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right]\\\\\n",
    " &= \\sum_{t=0}^\\infty \\mathbb{E}_{(s_i,a_i)_{i\\in [0,t]}} \\left[ \\left(\\sum_{t'=0}^{t-1} \\gamma^{t'} r(s_{t'},a_{t'})\\right) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right]\\\\\n",
    "  &= \\sum_{t=0}^\\infty \\mathbb{E}_{s_t,a_t | (s_i,a_i)_{i\\in [0,t-1]}} \\left[ \\mathbb{E}_{(s_i,a_i)_{i\\in [0,t-1]}} \\left[ \\sum_{t'=0}^{t-1} \\gamma^{t'} r(s_{t'},a_{t'})\\right] \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right]\\\\\n",
    "  &= 0\n",
    "\\end{align*}\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e99fc-fbe8-47d7-ab52-17a7c9df5ce6",
   "metadata": {},
   "source": [
    "So,\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s_i,a_i)_{i\\in \\mathbb{N}}} \\left[ \\sum_{t=0}^\\infty \\left(\\sum_{t'=t}^\\infty \\gamma^{t'} r(s_{t'},a_{t'}) \\right) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right] .$$\n",
    "\n",
    "And, factoring out the $\\gamma^t$,\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(s_i,a_i)_{i\\in \\mathbb{N}}} \\left[ \\sum_{t=0}^\\infty \\gamma^t \\left(\\sum_{t'=t}^\\infty \\gamma^{t'-t} r(s_{t'},a_{t'}) \\right) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right] .$$\n",
    "\n",
    "At this stage, it is important to recognize the random variable $G^\\pi(s)$ in this expression. Indeed, the sum $\\sum_{t'=t}^\\infty \\gamma^{t'-t} r(s_{t'},a_{t'})$ is exactly $G^\\pi(s_t)$. Similarly, one can recognize $G^\\pi(s,a)$, keeping in mind that $a$ is drawn according to $\\pi(s)$ anyway.\n",
    "\n",
    "This yields a finer expression for the policy gradient.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Monte Carlo policy gradient**  \n",
    "Given trajectories $\\tau=(s_t,a_t)_{t\\in [0,\\infty]}$ drawn according to policy $\\pi$, \n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau} \\left[ \\sum_{t=0}^\\infty \\gamma^t G_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\right],$$\n",
    "where $G_t = G^\\pi(s_t,a_t) = \\sum_{t'\\geq t} \\gamma^{t'-t} r_{t'}$ is the return random variable from the state-action pair visited at time step $t$\n",
    "</div>\n",
    "\n",
    "This estimator is called **Monte Carlo** because it relies on a (Monte Carlo) simulation of $\\pi$ to obtain realizations of $G^\\pi(s,\\pi(s))$ in the states visited by $\\pi$, and then uses these realizations as weights of $\\nabla \\log \\pi(a|s)$ in each encountered $(s,a)$ pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e64603-1355-430d-ad03-66fe804d1517",
   "metadata": {},
   "source": [
    "**Interpretation**  \n",
    "Let us rephrase and refine the previous interpretation.  \n",
    "Consider a fixed $s$. Then (again) $\\nabla_\\theta \\log \\pi(a|s)$ is the answer to \"in what direction should I change $\\theta$ to increase the log probability of taking action $a$ in $s$?\".   \n",
    "The expression above tells us that in order to improve the value of $\\pi_\\theta$, we should change $\\theta$ in a direction that is a linear combination of all $\\nabla_\\theta \\log\\pi(a|s)$, giving more weight to actions that provided large realizations of $G^\\pi(s,\\pi(s))$. \n",
    "\n",
    "We can push the interpretation of $\\nabla_\\theta \\log \\pi(a|s) = \\frac{\\nabla_\\theta \\pi(a|s)}{\\pi(a|s)}$ a bit further. $\\nabla_\\theta \\pi(a_t|s_t)$ is a vector in parameter space that points in the direction of greatest increase of $\\pi(a|s)$. The update will encourage taking a step in this direction if the action provided high return (through $G^\\pi(s,\\pi(s))$), but moving in this direction will be moderated if the action is already picked frequently (through $\\pi(a|s)$) so that other actions have a chance also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b455538-de3b-4b9b-a4e5-1783f3dd7acd",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "To compute the gradient estimator introduced previously, we can run the policy within the environment, and this will provide us with trajectories of states and actions following the distribution of $\\tau$ under $\\pi$. The full trajectory of states-actions-rewards provides a Monte Carlo estimate $G_t$ of $G^\\pi(s_t,a_t)$ from any state-action pair $(s_t,a_t)$ traversed by the trajectory. In turn, this allows to estimate $G_t \\nabla_\\theta \\log \\pi(a_t|s_t)$ for any of these pair. The sum over all states provides the gradient estimate.\n",
    "\n",
    "This algorithm, introduced by [Williams (1992)](https://link.springer.com/article/10.1007/BF00992696) is called REINFORCE. It requires a finite-length trajectory and its pseudo-code goes as follows.\n",
    "1. Initialize policy parameter $\\theta$\n",
    "2. Generate a trajectory by playing $\\pi$: $s_0,a_0,r_0,...s_{T}$\n",
    "3. For $t\\in [1, 2, … , T]$:\n",
    "    1. Estimate return $G_t$\n",
    "    2. Update policy parameter: $\\theta \\leftarrow \\theta + \\alpha \\gamma^t G_t \\nabla_\\theta \\log \\pi(a_t|s_t)$\n",
    "\n",
    "Historial note: REINFORCE is not just about \"reinforcing good trajectories\", it is an acronym that actually stands for \"REward Increment = Nonnegative Factor x Offset Reinforcement x Characteristic Eligibility\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b69a0-10b7-44a1-8ae1-2f6bf2236170",
   "metadata": {},
   "source": [
    "Modern deep learning libraries don't take gradients as inputs. Instead, they use automated differentiation to compute them for a given expression, given a batch of samples. So we would like to build an objective function such that its gradient estimated on the minibatch composed of the last trajectory's samples $(s,a)$ corresponds to the expression above.\n",
    "\n",
    "So we want to define $\\ell(\\theta)$ such that $-\\nabla_\\theta \\mathbb{E}_{s,a} \\left[ \\ell(\\theta,s,a) \\right]$ coincides with $\\nabla_\\theta J(\\theta)$. Note that, computationally, the $\\gamma^t G_t$ terms are just coefficients in the gradient estimate. So, very simply, taking $\\ell(\\theta) = q(s,a) \\cdot  \\log \\pi(a|s)$ achieves this goal as long as $q(s,a)$ matches the value of $\\gamma^t G_t$. \n",
    "\n",
    "**Super important remark: the loss is not a performance metric.**  \n",
    "In supervised learning, it is a common practice to monitor the empirical risk along training. \n",
    "But $L(\\theta) = -\\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ \\ell(\\theta,s,a) \\right]$ is not comparable to the losses one usually encounters in supervised learning in any way!  \n",
    "In particular, $-L(\\theta) \\neq J(\\theta)$. So it makes absolutely no sense monitoring the value of this artificial \"loss\" function along training, and even less interpreting its value as some performance metric. It is *just* an expression whose gradient coincides with the (opposite of the) policy gradient.\n",
    "\n",
    "**On sample efficiency.**  \n",
    "One key feature of policy gradient algorithms is that they are on-policy: they require the data to have been collected by the current policy, and discard this data once a gradient step is taken. Although this might seem sample inefficient, it can turn out to be an acceptable compromise if the policy gradient steps take the policy towards good returns quickly enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91550080-e2b1-44d0-817b-8b78580fa619",
   "metadata": {},
   "source": [
    "Let's implement REINFORCE for discrete action spaces.\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Declare a neural network approximator for a categorical distribution for $\\pi(a|s)$.  \n",
    "The `forward` method should output action probabilities.  \n",
    "Define a `sample_action` method that draws from the actions probabilities in state $s$.  \n",
    "Define a `log_prob` method that returns the log probability of an $(s,a)$ pair.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77f71429-6009-4ad3-842d-7ce51919a13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emmanuel/anaconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class policyNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        n_action = env.action_space.n\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, n_action)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(dim=0)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        action_scores = self.fc2(x)\n",
    "        return F.softmax(action_scores,dim=1)\n",
    "\n",
    "    def sample_action(self, x):\n",
    "        probabilities = self.forward(x)\n",
    "        action_distribution = Categorical(probabilities)\n",
    "        return action_distribution.sample().item()\n",
    "\n",
    "    def log_prob(self, x, a):\n",
    "        probabilities = self.forward(x)\n",
    "        action_distribution = Categorical(probabilities)\n",
    "        return action_distribution.log_prob(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f4dd14-c15b-4937-8c5f-90511d27b57e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Define a class that implements REINFORCE. Instead of drawing a single trajectory as in the pseudo-code above, include an option to draw several trajectories with the same policy. This will better the distribution of $\\tau$ and provide less noisy gradients.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760f6951-aba0-42e5-9fd8-ff3bfd43a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "\n",
    "class reinforce_agent:\n",
    "    def __init__(self, config, policy_network):\n",
    "        self.device = \"cuda\" if next(policy_network.parameters()).is_cuda else \"cpu\"\n",
    "        self.scalar_dtype = next(policy_network.parameters()).dtype\n",
    "        self.policy = policy_network\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.99\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.optimizer = torch.optim.Adam(list(self.policy.parameters()),lr=lr)\n",
    "        self.nb_episodes_per_gradient_step = config['nb_episodes_per_gradient_step'] if 'nb_episodes_per_gradient_step' in config.keys() else 1\n",
    "    \n",
    "    def one_gradient_step(self, env):\n",
    "        # run trajectories until done\n",
    "        episodes_sum_of_rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        coeffs = []\n",
    "        for ep in range(self.nb_episodes_per_gradient_step):\n",
    "            x,_ = env.reset()\n",
    "            rewards = []\n",
    "            episode_cum_reward = 0\n",
    "            while(True):\n",
    "                a = self.policy.sample_action(torch.as_tensor(x))\n",
    "                y,r,done,trunc,_ = env.step(a)\n",
    "                states.append(x)\n",
    "                actions.append(a)\n",
    "                rewards.append(r)\n",
    "                episode_cum_reward += r\n",
    "                x=y\n",
    "                if done: \n",
    "                    # The condition above should actually be \"done or trunc\" so that we \n",
    "                    # terminate the rollout also if trunc=True.\n",
    "                    # But then, our return-to-go computation would be biased as we would \n",
    "                    # implicitly assume no rewards can be obtained after truncation, which \n",
    "                    # is wrong.\n",
    "                    # We leave it as is for now (which means we will call .step() even \n",
    "                    # after trunc=True) and will discuss it later.\n",
    "                    # Compute returns-to-go\n",
    "                    discounted_returns = []\n",
    "                    G_t = 0\n",
    "                    for t,r in reversed(list(enumerate(rewards))):\n",
    "                        G_t = r + self.gamma * G_t\n",
    "                        discounted_returns.append(self.gamma**t * G_t)\n",
    "                    discounted_returns = list(reversed(discounted_returns))\n",
    "                    coeffs.extend(discounted_returns)\n",
    "                    episodes_sum_of_rewards.append(episode_cum_reward)\n",
    "                    break\n",
    "        # make loss\n",
    "        coeffs = torch.tensor(coeffs)\n",
    "        log_prob = self.policy.log_prob(torch.as_tensor(np.array(states)),torch.as_tensor(np.array(actions)))\n",
    "        loss = -(coeffs * log_prob).mean()\n",
    "        # gradient step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return np.mean(episodes_sum_of_rewards)\n",
    "\n",
    "    def train(self, env, nb_gradient_steps):\n",
    "        avg_sum_rewards = []\n",
    "        for ep in trange(nb_gradient_steps):\n",
    "            avg_sum_rewards.append(self.one_gradient_step(env))\n",
    "        return avg_sum_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c7c7a8-3cd7-4294-aaeb-62c0f42e248f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Run your agent for 50 gradient steps on `CartPole-v1`, drawing 10 episodes at each step.  \n",
    "Experiment with another number of episodes and vary the gradient step size.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51a0c209-97bd-49fd-9dad-c08d6abb3a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:31<00:00,  1.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f239eeade20>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "config = {'gamma': .99,\n",
    "          'learning_rate': 0.01,\n",
    "          'nb_episodes_per_gradient_step': 10\n",
    "         }\n",
    "\n",
    "pi = policyNetwork(env)\n",
    "agent = reinforce_agent(config, pi)\n",
    "returns = agent.train(env,50)\n",
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0827e2da-5f69-4723-81ab-c2be40317fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "#test_env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array_list\")\n",
    "test_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
    "s,_ = test_env.reset()\n",
    "with torch.no_grad():\n",
    "    for t in range(1000):\n",
    "        a = pi.sample_action(torch.as_tensor(s))\n",
    "        s2,r,d,trunc,_ = test_env.step(a)\n",
    "        s = s2\n",
    "        if d:\n",
    "            break\n",
    "\n",
    "save_video(test_env.render(), \"videos\", fps=test_env.metadata[\"render_fps\"], name_prefix=\"reinforce_policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8c1aed4-e7b1-45e8-af8f-19bd26157b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"videos/reinforce_policy-episode-0.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"videos/reinforce_policy-episode-0.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477711f2-0f4b-4375-83dc-cf35a3b7ffc5",
   "metadata": {},
   "source": [
    "**Comment on the behavior of policy gradient methods (based on the CartPole example).**\n",
    "\n",
    "One very interesting feature of policy gradient methods, compared to dynamic programming ones, is that they take a somewhat more pragmatic  and easier approach at policy improvement.\n",
    "\n",
    "Policy gradients make the policy better by reinforcing good actions in visited states, and discouraging actions that led to bad outcomes. In CartPole, this means that exiting the screen on either side will be penalized as much as falling. This happens quite early in the optimization process: as soon as the policy balances the pole, it starts to drift towards the sides of the screen, gets this negative reward, and hence reinforces actions that keep the cart within the screen (the fact that we draw several trajectories for each policy helps a lot for that matter since it enables comparing different actions and reinforcing the ones which belonged to better trajectories).\n",
    "\n",
    "Converserly, maybe you remember DQN did a great job at quickly learning how to balance the pole, but wasn't so great at avoiding drifting to the edges of the screen. The key difference is that DQN stored all past experience, and that exiting from the side ended up being a rather rare situation within the replay buffer. Also, DQN needed to propagate these bad rewards through the value function (a task greatly enhanced by methods like prioritized experience replay), which is done immediately in REINFORCE because $G_t$ is a Monte Carlo estimator, not a TD(0) one.\n",
    "\n",
    "So maybe, with ADP methods, we had set up a task that was excessively difficult. The ADP approach is elegant and sample efficient (it retains a long-term memory of samples and solves for the optimal $Q$ value for *all* state and action pairs. But maybe this was more than what was necessary: a local search among policy parameters, without retaining a memory of past experience, but instead local comparison of trajectories and reinforcement of good ones, might be sufficient to achieve efficient learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91544b34-5c06-45d5-9424-46b4916afafc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "The code above is slightly inefficient as it passes each state through the network twice: one when drawing the action (`sample_action`), the other when computing log probabilities (`log_prob`). This is not critical as the latter is a batched version of this forward pass, but still, we can optimize this a bit. Make this a bit more elegant by writing a `sample_action_and_log_prob` function in the `reinforce_agent` class, which jointly draws the action and computes its log probability, enabling storing the log probabilities along training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d797ec8-ab32-4779-86de-714055e0befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "\n",
    "class reinforce_agent:\n",
    "    def __init__(self, config, policy_network):\n",
    "        self.device = \"cuda\" if next(policy_network.parameters()).is_cuda else \"cpu\"\n",
    "        self.scalar_dtype = next(policy_network.parameters()).dtype\n",
    "        self.policy = policy_network\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.99\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.optimizer = torch.optim.Adam(list(self.policy.parameters()),lr=lr)\n",
    "        self.nb_episodes_per_gradient_step = config['nb_episodes_per_gradient_step'] if 'nb_episodes_per_gradient_step' in config.keys() else 1\n",
    "\n",
    "    def sample_action_and_log_prob(self, x):\n",
    "        probabilities = self.policy(torch.as_tensor(x))\n",
    "        action_distribution = Categorical(probabilities)\n",
    "        action = action_distribution.sample()\n",
    "        log_prob = action_distribution.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "    \n",
    "    def one_gradient_step(self, env):\n",
    "        # run trajectories until done\n",
    "        episodes_sum_of_rewards = []\n",
    "        log_probs = []\n",
    "        coeffs = []\n",
    "        for ep in range(self.nb_episodes_per_gradient_step):\n",
    "            x,_ = env.reset()\n",
    "            rewards = []\n",
    "            episode_cum_reward = 0\n",
    "            while(True):\n",
    "                a, log_prob = self.sample_action_and_log_prob(x)\n",
    "                y,r,d,_,_ = env.step(a)\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(r)\n",
    "                episode_cum_reward += r\n",
    "                x=y\n",
    "                if d:\n",
    "                    # compute returns-to-go\n",
    "                    discounted_returns = []\n",
    "                    G_t = 0\n",
    "                    for t,r in reversed(list(enumerate(rewards))):\n",
    "                        G_t = r + self.gamma * G_t\n",
    "                        discounted_returns.append(self.gamma**t * G_t)\n",
    "                    discounted_returns = list(reversed(discounted_returns))\n",
    "                    coeffs.extend(discounted_returns)\n",
    "                    episodes_sum_of_rewards.append(episode_cum_reward)\n",
    "                    break\n",
    "        # make loss\n",
    "        coeffs = torch.tensor(coeffs)\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        loss = -(coeffs * log_probs).mean()\n",
    "        # gradient step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return np.mean(episodes_sum_of_rewards)\n",
    "\n",
    "    def train(self, env, nb_gradient_steps):\n",
    "        avg_sum_rewards = []\n",
    "        for ep in trange(nb_gradient_steps):\n",
    "            avg_sum_rewards.append(self.one_gradient_step(env))\n",
    "        return avg_sum_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3684a5e-5e6e-4132-9267-9745c2e46a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:40<00:00,  1.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f23971d0ad0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "config = {'gamma': .99,\n",
    "          'learning_rate': 0.01,\n",
    "          'nb_episodes_per_gradient_step': 10\n",
    "         }\n",
    "\n",
    "pi = policyNetwork(env)\n",
    "agent = reinforce_agent(config, pi)\n",
    "returns = agent.train(env,50)\n",
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abb18f8-66b4-4d78-bbe2-7a77269774d4",
   "metadata": {},
   "source": [
    "**The problem of rollout terminations.**\n",
    "\n",
    "One weakness of REINFORCE is that it requires trajectories that terminate in finite time. This was reasonable enough in the previous example as we let the environment run until `done=True`, but this does not happen in all MDPs. Some environments have no particular termination condition. When we instantiate such environments, we generally set a time limit, but this does not constitute a termination condition, rather a trajectory truncation one. In particular, a Monte Carlo estimate of $\\mathbb{E}[G^\\pi(s,a)]$ based on a truncated trajectory might be very biased if $s,a$ is encountered at a time step close to the time limit.\n",
    "\n",
    "With this problem, we reach the limit of Monte Carlo evaluations. In previous chapters, to counter this problem, we have introduced value function estimators and bootstrapping (TD learning of Q-functions being the off-policy, 1-step bootstraping version)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb015050-8c21-4bb6-81b0-91a9fb0e4869",
   "metadata": {},
   "source": [
    "# Actor-critic algorithms\n",
    "\n",
    "## Introducing a critic\n",
    "\n",
    "Recall that REINFORCE uses a Monte Carlo estimate of the policy gradient. With a slight notation abuse, we have:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**REINFORCE:**\n",
    "$$\\tilde{\\nabla}_\\theta J(\\theta) = \\mathbb{E}_{\\tau} \\left[ G_t \\nabla_\\theta \\log\\pi(a|s)\\right]$$\n",
    "</div>\n",
    "\n",
    "**Actor-critic algorithms**  \n",
    "Suppose now that we don't want a Monte Carlo estimate of $Q^\\pi(s,a)$ in the Policy Gradient theorem, and are rather willing to store a function approximator $Q_w$ for $Q^\\pi(s,a)$. This leads us to store both a policy $\\pi_\\theta$ and a value function $Q_w$. The value function *criticizes* the policy's selected actions by assigning numerical values to them, hence the names of *critic* and *actor*. So actor-critic algorithms are policy gradient algorithms that use an actor-critic architecture (remember: the opposite is not necessarily true).\n",
    "\n",
    "The $Q_w$ function of the critic can be learned as a risk minimization problem (eg. a least squares fitting problem), using temporal differences. Note that although this learning is off-policy, the data collection for the application of the policy gradient theorem still requires on-policy data.\n",
    "\n",
    "This defines the Q actor-critic algorithm:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Q actor-critic:**\n",
    "$$\\tilde{\\nabla}_\\theta J(\\theta) = \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ Q_w(s,a) \\nabla_\\theta \\log\\pi(a|s)\\right]$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495bd5f9-446b-49d1-9f55-3b59543cdf5a",
   "metadata": {},
   "source": [
    "## Baselines in policy gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a8ca6f-0bb6-4ad3-89b7-6b3948522871",
   "metadata": {},
   "source": [
    "**Finite sample gradient estimator variance**\n",
    "\n",
    "The policy gradient features a somehow counter-intuitive property for finite sample estimators.  \n",
    "Suppose we have a 3 time steps trajectory, drawn following $\\pi$, with, $G_0 = 1$, $\\gamma G_1 = 10$, and $\\gamma^2 G_2 = 1$.\n",
    "\n",
    "For the sake of brevity, let's write $\\nabla_i = \\nabla_\\theta \\log \\pi(a_i|s_i)$. \n",
    "So the finite sample Monte Carlo estimator for $\\nabla_\\theta J(\\theta)$ is:\n",
    "$$1\\nabla_0 + 10\\nabla_1 + 1\\nabla_2.$$\n",
    "\n",
    "Following this update of $\\theta$, we will hopefully see trajectories go through $s_1,a_1$ a lot more often, and through $s_0,a_0$ and $s_2,a_2$ a little bit more often.\n",
    "\n",
    "Now recall the expected grad-log-prob lemma: we can substract any action-independent *baseline* $b(s)$ from the realization of $G^\\pi(s,a)$ and still obtain an unbiased estimate of $\\nabla_\\theta J(\\theta)$, because $\\mathbb{E}_{\\tau} \\left[b(s) \\nabla_\\theta \\log\\pi(a|s) \\right]=0$:\n",
    "$$\\nabla_\\theta J(\\theta) \\propto \\mathbb{E}_{\\tau} \\left[ \\left( \\gamma^t G_t -b(s) \\right) \\nabla_\\theta \\log\\pi(a|s)\\right].$$\n",
    "\n",
    "Let's add a constant value of $b(s)=-1$ to all returns. Then our finite sample estimate becomes:\n",
    "$$9\\nabla_1.$$\n",
    "Let's use $b(s)=-10$.  Then our estimated ascent direction is:\n",
    "$$-9\\nabla_0 -9\\nabla_2.$$\n",
    "Conversely, let's add 20 to all returns. Then the estimate gradient is:\n",
    "$$21 \\nabla_0 + 30\\nabla_1 + 21\\nabla_2.$$\n",
    "\n",
    "We have not changed our MDP, nor our policy, but the gradient estimate is drastically different, both in norm and in direction! So what has happened?\n",
    "\n",
    "What has happened is that we have a **finite** sample of states and actions. The baseline $b(s)$ does not affect the gradient estimate's average in the limit of **infinite** sampling. But depending on the baseline's value, the finite sample estimator will have more or less **variance**.\n",
    "\n",
    "In the previous example, if we augment the probability of $a$ in a given $s$, then we need to decrease it for another $a'$. If this other $a'$ has not been sampled, then its $\\nabla\\theta \\log\\pi_\\theta(a'|s)$ cannot participate in the estimated ascent direction which might be off just because it strongly depends on which sample set was drawn.\n",
    "\n",
    "Ideally, we would like to find a baseline which minimizes variance of the estimator, so that whatever finite sample set we draw, we have a good chance of having a good quality gradient estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42143dbb-d19f-4172-82f8-49465c30d47d",
   "metadata": {},
   "source": [
    "**An optimal baseline?**\n",
    "\n",
    "Let's search for an optimal baseline $b(s)$ in each state, by writing the variance of the gradient estimate. In each state $s$, we write:\n",
    "$$d(s) = \\mathbb{E}_{a\\sim \\pi} \\left[ \\left( Q^\\pi(s,a) - b(s) \\right) \\nabla_\\theta \\log\\pi(a|s)\\right].$$\n",
    "As stated before, the estimated ascent direction is $\\mathbb{E}_{s\\sim \\rho^\\pi} [d(s)]$.\n",
    "\n",
    "To simplify notations, we will drop all mentions of $s$ ($s$ is fixed), and will write $\\nabla(a)$ for $\\nabla_\\theta \\log\\pi(a|s)$. Let us define a proxy for the variance of vector $d$:\n",
    "$$Var[d] = \\mathbb{E}_{a\\sim \\pi} \\left[ \\left( Q^\\pi(a)-b \\right)^2 \\nabla(a)^2 \\right] - \\left[ \\mathbb{E}_{a\\sim \\pi} \\left[ \\left( Q^\\pi(a)-b \\right) \\nabla(a) \\right] \\right]^2.$$\n",
    "(this is related to the trace of the covariance matrix but we won't go into details on that for now)\n",
    "\n",
    "We wish to find $b$ that minimizes $Var[d]$. A first remark is that the second term is actually equal to $\\left[ \\mathbb{E}_{a\\sim \\pi} \\left[ \\left( Q^\\pi(a) \\right) \\nabla(a) \\right] \\right]^2$ and does not depend on $b$. We get that:\n",
    "\\begin{align*}\n",
    "\\frac{\\mathrm{d} Var[d]}{\\mathrm{d}b} &= \\frac{\\mathrm{d}}{\\mathrm{d}b} \\mathbb{E}_{a\\sim \\pi} \\left[ \\left( Q^\\pi(a)-b \\right)^2 \\nabla(a)^2 \\right],\\\\\n",
    " &= \\frac{\\mathrm{d}}{\\mathrm{d}b} \\Big[\\mathbb{E}_{a\\sim \\pi} \\left[Q^\\pi(a)^2 \\nabla(a)^2\\right] -2 b\\mathbb{E}_{a\\sim \\pi} \\left[Q^\\pi(a) \\nabla(a)\\right] + b^2 \\mathbb{E}_{a\\sim \\pi} \\left[\\nabla(a)^2\\right] \\Big],\\\\\n",
    " &= -2 \\mathbb{E}_{a\\sim \\pi} \\left[Q^\\pi(a) \\nabla(a)^2\\right] + 2 \\mathbb{E}_{a\\sim \\pi} \\left[b\\nabla(a)^2\\right].\n",
    "\\end{align*}\n",
    "\n",
    "And so the value that cancels this gradient is:\n",
    "$$b = \\frac{\\mathbb{E}_{a\\sim \\pi} \\left[Q^\\pi(a) \\nabla(a)^2\\right]}{\\mathbb{E}_{a\\sim \\pi} \\left[\\nabla(a)^2\\right]}.$$\n",
    "\n",
    "Suppose $\\nabla(a)^2$ is evenly distributed for all $a$. Then this fraction boils down to $b = \\mathbb{E}_{a\\sim \\pi} \\left[Q^\\pi(a)\\right]$, which means $b(s) = V^\\pi(s)$. \n",
    "\n",
    "But there is no reason for $\\nabla(a)^2$ is evenly distributed for all $a$. The optimal baseline $b(s)$ is a weighted average across actions of $Q^\\pi(s,a)$ but with actions sampled according to a distribution which depends both on $\\pi(a|s)$ and $\\nabla_\\theta \\log\\pi(a|s)$. In other words: it's almost $V^\\pi(s)$ but instead of weighting every action with $\\pi(a|s)$, it is additionally weighted by the norm of the gradient of the policy's log probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6815ff92-f505-4a2c-9443-41a9a770e3fe",
   "metadata": {},
   "source": [
    "**The value function as a baseline**\n",
    "\n",
    "Although it is not optimal, a close enough choice for a baseline is the policy's value function $V^\\pi$. This introduces an *advantage* estimation problem within policy gradient algorithms, where the advantage is the function defined as:$$A^\\pi(s,a) = Q^\\pi(s,a) - V^\\pi(s).$$\n",
    "\n",
    "Remember that $Q^\\pi(s,a) = \\mathbb{E}_{s'\\sim p(s,a)} [r(s,a,s') + \\gamma V^\\pi(s')]$. So, given a sample $(s,a,r,s')$, a sample estimate of the advantage function is $r + \\gamma V^\\pi(s') - V^\\pi(s)$. So learning $V^\\pi$ provides a way to obtain sample estimates of the advantage function.\n",
    "\n",
    "This yields the advantage actor critic algorithm:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Advantage actor-critic:**\n",
    "$$\\tilde{\\nabla}_\\theta J(\\theta) = \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ A_w(s,a) \\nabla_\\theta \\log\\pi(a|s)\\right]$$\n",
    "$$\\tilde{\\nabla}_\\theta J(\\theta) = \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ \\mathbb{E}_{s'\\sim p(s,a)} \\left[r(s,a,s') + \\gamma V_w(s') - V_w(s)\\right] \\nabla_\\theta \\log\\pi(a|s)\\right]$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daa9795-dd0b-4912-8b62-3b7ed16d7c79",
   "metadata": {},
   "source": [
    "Remark that the temporal difference at each time step $\\delta = r + \\gamma V^\\pi(s') - V^\\pi(s)$ is an estimate of the advantage $A^\\pi(s,a)$. Using this remark, a simple one-step advantage actor-critic method based on TD(0) and a value function $V_w$ goes as follows:\n",
    "1. In $s$, draw $a \\sim \\pi$\n",
    "2. Observe $r, s'$\n",
    "3. Compute $\\delta = r + \\gamma V_w(s') - V_w(s)$\n",
    "4. Update critic's parameters (TD(0) step) $w \\leftarrow w + \\alpha \\delta \\nabla_w V_w(s)$\n",
    "5. Update actor's parameters (policy gradient theorem) $\\theta \\leftarrow \\theta + \\alpha \\delta \\nabla_\\theta \\log \\pi(a|s)$\n",
    "6. $s\\leftarrow s'$ and repeat\n",
    "\n",
    "This directly translates into a batch algorithm:\n",
    "1. Sample trajectories $\\{s_t,a_t,r_t,s'_t\\}$ from $\\pi$\n",
    "2. Fit $V_w$ to the (bootstrapped) returns $r_t + \\gamma V_w(s'_t)$\n",
    "3. Get advantage $A(s_t,a_t) = r_t + \\gamma V_w(s'_t) - V_w(s_t)$\n",
    "4. Update policy using ascent direction $\\sum_t A(s_t,a_t) \\nabla_\\theta \\log \\pi(a_t|s_t)$\n",
    "\n",
    "An interesting feature of this algorithm is that it does not require to store experience samples from past policies. Since fitting of $V_w$ is an on-policy problem, it operates on the last collected batch of samples and does not retain the experience replay buffers we encountered with ADP methods.\n",
    "\n",
    "Overfitting of $V_w$ to the samples it is trained upon is not an issue since $V_w$ will only ever be evaluated on these same samples.\n",
    "\n",
    "To reduce the bias in learning $V_w$, instead of one-step targets $r_t + \\gamma V_w(s'_t)$, one can store and use n-step returns $\\sum_{t'=t}^{t+n-1} \\gamma^{t'-t} r_{t'} + \\gamma^{n} V_w(s'_{t+n-1})$ for each $s_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514bc5cb-278a-4a72-a51f-51ba17acf2c0",
   "metadata": {},
   "source": [
    "## A2C: Deep Advantage Actor-Critic\n",
    "\n",
    "In 2016, Mnih et al. introduced a set of algorithms using asynchronous environments in the **[Asynchronous Methods for Deep Reinforcement Learning](https://arxiv.org/abs/1602.01783)** paper. Among these methods, one is called A3C for Asynchronous Advantage Actor-Critic. \n",
    "In a nutshell, A3C is an advantage actor-critic method where multiple policy workers in parallel environments independently update a global value function. One supposed key benefit of having asynchronous actors was to promote exploration of the state space. It later turned out, the full asynchronicity of independent policies did not bring a significant performance advantage while inducing some computational overhead. A2C is A3C's little sibling, using a single (hence synchronous) policy.\n",
    "\n",
    "At each policy update step, A2C runs a number of rollouts with $\\pi_\\theta$, collecting samples.\n",
    "\n",
    "It uses bootstrapped Monte Carlo returns to estimate the advantage in each state-action pair:\n",
    "$$A(s_t,a_t;w) = \\sum_{t'=t}^{T-1} \\gamma^{t'-t} r_{t'} + \\gamma^{T-t} V_w(s_{T}) - V_w(s_t),$$\n",
    "where $T$ is the termination time of a rollout.\n",
    "\n",
    "The value function learning takes a gradient step to minimize:\n",
    "$$L(w) = \\sum_t A(s_t,a_t;w)^2.$$\n",
    "\n",
    "The policy learning step takes a gradient step to maximize:\n",
    "$$L(\\theta)=\\sum_t \\left[ A(s_t,a_t;w) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) + \\beta \\nabla_\\theta \\mathcal{H}(\\pi_\\theta(s_t) \\right].$$\n",
    "The first term in this sum is the usual policy gradient. The second one is the entropy of the policy. The intention is to promote exploration by retaining randomness in the policy.\n",
    "\n",
    "So, in short, A2C's pseudo code is:\n",
    "1. Collect trajectories $\\{s_t,a_t,r_t,s'_t\\}$ from $\\pi$\n",
    "2. Define $A(s_t,a_t;w) = \\sum_{t'=t}^{T-1} \\gamma^{t'-t} r_{t'} + \\gamma^{T-t} V_w(s_{t+T}) - V_w(s_t)$\n",
    "3. Take a gradient descent step on $L(w)$ and a gradient ascent step on $L(\\theta)$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Declare a critic network.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b99bdc1-fa2c-4fe0-b670-42c3d44b471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class valueNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        n_action = env.action_space.n\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(dim=0)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f171a971-6013-4701-bcf1-369f5663118a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Implement an A2C algorithm.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c5078d5-8092-47ad-84ac-d3e99dc19926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "\n",
    "class a2c_agent:\n",
    "    def __init__(self, config, policy_network, value_network):\n",
    "        self.device = \"cuda\" if next(policy_network.parameters()).is_cuda else \"cpu\"\n",
    "        self.scalar_dtype = next(policy_network.parameters()).dtype\n",
    "        self.policy = policy_network\n",
    "        self.value = value_network\n",
    "        self.gamma = config['gamma'] if 'gamma' in config.keys() else 0.99\n",
    "        lr = config['learning_rate'] if 'learning_rate' in config.keys() else 0.001\n",
    "        self.optimizer = torch.optim.Adam(list(self.policy.parameters()) + list(self.value.parameters()),lr=lr)\n",
    "        self.nb_episodes = config['nb_episodes'] if 'nb_episodes' in config.keys() else 1\n",
    "        self.entropy_coefficient = config['entropy_coefficient'] if 'entropy_coefficient' in config.keys() else 0.001\n",
    "\n",
    "    def sample_action(self, x):\n",
    "        probabilities = self.policy(torch.as_tensor(x))\n",
    "        action_distribution = Categorical(probabilities)\n",
    "        action = action_distribution.sample()\n",
    "        log_prob = action_distribution.log_prob(action)\n",
    "        entropy = action_distribution.entropy()\n",
    "        return action.item(), log_prob, entropy\n",
    "    \n",
    "    def one_gradient_step(self, env):\n",
    "        # run trajectories until done\n",
    "        episodes_sum_of_rewards = []\n",
    "        log_probs = []\n",
    "        returns = []\n",
    "        values = []\n",
    "        entropies = []\n",
    "        for ep in range(self.nb_episodes):\n",
    "            x,_ = env.reset()\n",
    "            rewards = []\n",
    "            episode_cum_reward = 0\n",
    "            while(True):\n",
    "                a, log_prob, entropy = self.sample_action(x)\n",
    "                y,r,d,trunc,_ = env.step(a)\n",
    "                values.append(self.value(torch.as_tensor(x)).squeeze(dim=0))\n",
    "                log_probs.append(log_prob)\n",
    "                entropies.append(entropy)\n",
    "                rewards.append(r)\n",
    "                episode_cum_reward += r\n",
    "                x=y\n",
    "                if d or trunc:\n",
    "                    # compute returns-to-go\n",
    "                    new_returns = []\n",
    "                    G_t = torch.tensor([0])\n",
    "                    if trunc:\n",
    "                        G_t = self.value(torch.as_tensor(x)).squeeze(dim=0)\n",
    "                    for r in reversed(rewards):\n",
    "                        G_t = r + self.gamma * G_t\n",
    "                        new_returns.append(G_t)\n",
    "                    new_returns = list(reversed(new_returns))\n",
    "                    returns.extend(new_returns)\n",
    "                    episodes_sum_of_rewards.append(episode_cum_reward)\n",
    "                    break\n",
    "        # make loss        \n",
    "        returns = torch.cat(returns)\n",
    "        values = torch.cat(values)\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        entropies = torch.cat(entropies)\n",
    "        advantages = returns - values\n",
    "        pg_loss = -(advantages.detach() * log_probs).mean()\n",
    "        entropy_loss = -entropies.mean()\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        loss = pg_loss + critic_loss + self.entropy_coefficient * entropy_loss\n",
    "        # gradient step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return np.mean(episodes_sum_of_rewards)\n",
    "\n",
    "    def train(self, env, nb_rollouts):\n",
    "        avg_sum_rewards = []\n",
    "        for ep in trange(nb_rollouts):\n",
    "            avg_sum_rewards.append(self.one_gradient_step(env))\n",
    "        return avg_sum_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd5ea381-f556-4177-8864-228ffcf17e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:40<00:00,  1.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb9988cc710>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "config = {'gamma': .99,\n",
    "          'learning_rate': 0.01,\n",
    "          'nb_episodes': 10,\n",
    "          'entropy_coefficient': 1e-3\n",
    "         }\n",
    "\n",
    "pi = policyNetwork(env)\n",
    "V  = valueNetwork(env)\n",
    "agent = a2c_agent(config, pi, V)\n",
    "returns = agent.train(env,50)\n",
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3aac837a-0fe2-4a07-9df1-e0d29e642f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/emmanuel/git_repos/RLclass_MVA/notebooks/videos/a2c_policy-episode-0.mp4.\n",
      "Moviepy - Writing video /home/emmanuel/git_repos/RLclass_MVA/notebooks/videos/a2c_policy-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/emmanuel/git_repos/RLclass_MVA/notebooks/videos/a2c_policy-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "#test_env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array_list\")\n",
    "test_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array_list\")\n",
    "s,_ = test_env.reset()\n",
    "with torch.no_grad():\n",
    "    for t in range(1000):\n",
    "        a = pi.sample_action(torch.as_tensor(s))\n",
    "        s2,r,d,trunc,_ = test_env.step(a)\n",
    "        s = s2\n",
    "        if d:\n",
    "            break\n",
    "\n",
    "save_video(test_env.render(), \"videos\", fps=test_env.metadata[\"render_fps\"], name_prefix=\"a2c_policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c871de74-b3af-49dc-85bd-d0dc5a655915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"videos/a2c_policy-episode-0.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"videos/a2c_policy-episode-0.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773ca382-930f-4b18-9f7b-62c12253612a",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this chapter, we have considered the problem of direct policy search. We have:\n",
    "- cast this problem as a gradient ascent one,\n",
    "- introduced the policy gradient theorem,\n",
    "- derived its direct application as the REINFORCE algorithm,\n",
    "- tackled the problem of gradient variance through the introduction of baselines.\n",
    "- Since baselines required estimating a value function, we have written actor-critic algorithms,\n",
    "- and in particular we have introduced the recent A2C algorithm.\n",
    "\n",
    "Of course there are still lots of things to cover in the literature on policy gradients and more generally policy search. The homework will guide you through some of these important topics, which should provide you with an up-to-date view on the current state-of-the-art in direct policy search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43c7b87-6d9a-4a2b-ac8e-1c942d251978",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "The exercises below are here to help you play with the concepts introduced above, to better grasp them. They also introduce additional important notions. They are not optional to reach the class goals. Often, the provided answer reaches out further than the plain question asked and provides comments, additional insights, or external references."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f87e89-dbbc-4b1a-af70-c339ce26d137",
   "metadata": {},
   "source": [
    "## Running rollouts in parallel\n",
    "\n",
    "One key feature of policy gradient algorithms is that they are on-policy: they require the data to have been collected by the current policy, and discard this data once a gradient step is taken. Although this might seem sample inefficient, it can turn out to be an acceptable compromise if the policy gradient steps take the policy towards good returns quickly enough.\n",
    "\n",
    "A direct consequence of this on-policy property, is that policy gradients don't use experience replay buffers. Instead, they can take advantage of parallel computation to collect samples. We saw that running several rollouts with the same policy greatly helped in reducing the gradient's variance.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Gymnasium provides a [vectorized environments](https://gymnasium.farama.org/api/vector/) class, which enables running multiple independent copies of the same environment in parallel, for a certain number of steps. Modify the REINFORCE and A2C classes to use a vector of environments.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452a578-77e7-4b2e-9a2a-677391d41f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMO CODE OF VECTORIZED ENVIRONMENTS\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "## Make vector env and simulate one step\n",
    "#envs = gym.vector.make(\"CartPole-v1\", num_envs=10)\n",
    "envs = gym.make_vec(\"CartPole-v1\", num_envs=10)\n",
    "## Reset all environments\n",
    "states, _ = envs.reset(seed=42)\n",
    "#print(states)\n",
    "## Choose actions\n",
    "actions = envs.action_space.sample()\n",
    "#print(actions)\n",
    "## Step\n",
    "next_states, rewards, termination, truncation, infos = envs.step(actions)\n",
    "#print(observations)\n",
    "#print(infos)\n",
    "\n",
    "## Warning, when an environment terminates or is truncated, it automatically resets \n",
    "## so that you can keep calling .step() and it does not wait idly until others have \n",
    "## finished.\n",
    "## When this happens, the next state returned by .step() is the result of the reset\n",
    "## and the last state of the previous trajectory is accessed through the \n",
    "## infos['final_observation'] variable.\n",
    "## This also means you cannot use the termination or truncation flags anymore to run \n",
    "## post-trajectories computations.\n",
    "\n",
    "states = next_states\n",
    "for t in range(50):\n",
    "    actions = envs.action_space.sample()\n",
    "    next_states, rewards, termination, truncation, infos = envs.step(actions)\n",
    "    if any(termination):\n",
    "        index = np.where(termination == True)\n",
    "        print(\"Termination vector:\", termination, index)\n",
    "        index = index[0][0]\n",
    "        print(\"Env\", index, \"has been reset! Here is the last transition:\")\n",
    "        print(\"starting state =\", states[index])\n",
    "        print(\"ending state   =\", infos['final_observation'][index])\n",
    "        print(\"reset state    =\", next_states[index])\n",
    "        break\n",
    "    states = next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda50b5-2a62-4dd3-a8ec-374db6cac8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load solutions/no_solution_yet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf86032-a614-4552-a7f6-324e7be08002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "config = {'gamma': .99,\n",
    "          'learning_rate': 0.01,\n",
    "          'nb_episodes': 10,\n",
    "          'nb_sim_steps_per_update': 500\n",
    "         }\n",
    "\n",
    "pi = policyNetwork(env)\n",
    "agent = reinforce_agent(config, pi)\n",
    "returns = agent.train(env,50)\n",
    "plt.plot(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e06d3e2-608b-4823-b6b2-a4238a7e8c5d",
   "metadata": {},
   "source": [
    "## Policy gradients on continuous action domains\n",
    "\n",
    "REINFORCE and A2C with Gaussian policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a91884-4238-4c04-87fb-fc33d6c6040e",
   "metadata": {},
   "source": [
    "## Policy gradients for the finite horizon criterion\n",
    "\n",
    "Derive the PG for the total reward, finite horizon criterion.\n",
    "\n",
    "Implement a REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b11d3b9-3790-4df0-8ec5-7ea7ed265428",
   "metadata": {},
   "source": [
    "## Generalized advantage estimation\n",
    "\n",
    "[Generalized Advantage Estimation (Schulman et al., 2016)](https://arxiv.org/abs/1506.02438)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1e2ac1-5685-41f7-b3cd-114cc3c2212a",
   "metadata": {},
   "source": [
    "## From off-policy policy gradients to TRPO\n",
    "\n",
    "[Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)\n",
    "\n",
    "Policy gradient methods are intrinsically on-policy algorithms, as states and actions need to be sampled according to $\\rho^\\pi$ and $\\pi$ respectively. But could we make an off-policy version of this gradient estimator?\n",
    "\n",
    "More precisely, could we use data collected by some policy $\\pi$, and the corresponding advantage function $A^{\\pi}$, to still compute the gradient of $J(\\theta')$?\n",
    "\n",
    "In this section, we try to write an optimization problem whose gradient can be used to optimize $J(\\theta')$ while using samples from $\\pi_\\theta$.\n",
    "\n",
    "**$J(\\theta')$ as an expression of $\\pi$ and $A^\\pi$**\n",
    "\n",
    "Recall that:\n",
    "$$J(\\theta) = \\mathbb{E}_{s_0 \\sim p_0} \\left[ V^\\pi(s_0) \\right].$$\n",
    "\n",
    "One can remark that this is also:\n",
    "$$J(\\theta) = \\mathbb{E}_{\\substack{s_0 \\sim p_0\\\\ s_{t>0}\\sim p_t(\\cdot|\\pi')\\\\ a_t\\sim \\pi'}} \\left[ V^\\pi(s_0) \\right].$$\n",
    "\n",
    "Since the starting state distribution is assumed fixed, we can write:\n",
    "$$J(\\theta) = \\mathbb{E}_{\\substack{s_t\\sim p_t(\\cdot|\\pi')\\\\ a_t\\sim \\pi'}} \\left[ V^\\pi(s_0) \\right].$$\n",
    "\n",
    "It is tempting to compare this expression of $J(\\theta)$ which artificially uses $\\pi'$, with that of $J(\\theta')$ and make the advantage function of $\\pi$ appear, with the intention to write $J(\\theta')$ as an expression of $J(\\theta)$ and this advantage function. \n",
    "\n",
    "Let's first remark that:\n",
    "\\begin{align*}\n",
    "J(\\theta) &= \\mathbb{E}_{\\substack{s_t\\sim p_t(\\cdot|\\pi')\\\\ a_t\\sim \\pi'}} \\left[ \\sum_{t=0}^\\infty \\gamma^t V^\\pi(s_t) - \\sum_{t=1}^\\infty \\gamma^t V^\\pi(s_t) \\right],\\\\\n",
    " &= \\mathbb{E}_{\\substack{s_t\\sim p_t(\\cdot|\\pi')\\\\ a_t\\sim \\pi'}} \\left[ \\sum_{t=0}^\\infty \\gamma^t \\left( V^\\pi(s_t) - \\gamma V^\\pi(s_{t+1}) \\right) \\right].\n",
    "\\end{align*}\n",
    "\n",
    "Then recall that $J(\\theta') = \\mathbb{E}_{\\substack{s_t\\sim p_t(\\cdot|\\pi')\\\\ a_t\\sim \\pi'}} \\left[ \\gamma^t r(s_t,a_t) \\right]$. So:\n",
    "\\begin{align*}\n",
    "J(\\theta') - J(\\theta) &= \\mathbb{E}_{\\substack{s_t\\sim p_t(\\cdot|\\pi')\\\\ a_t\\sim \\pi'}} \\left[ \\sum_{t=0}^\\infty \\gamma^t \\left( r(s_t, a_t) + \\gamma V^\\pi(s_{t+1}) - V^\\pi(s_t) \\right) \\right],\\\\\n",
    " &= \\mathbb{E}_{\\substack{s_t\\sim p_t(\\cdot|\\pi')\\\\ a_t\\sim \\pi'}} \\left[ \\sum_{t=0}^\\infty \\gamma^t A^\\pi(s_t,a_t) \\right].\n",
    "\\end{align*}\n",
    "\n",
    "In other terms:\n",
    "$$J(\\theta') = J(\\theta) + \\mathbb{E}_{s_t\\sim p_t(\\cdot|\\pi')} \\left[ \\mathbb{E}_{a_t\\sim \\pi'} \\left[ \\sum_{t=0}^\\infty \\gamma^t A^\\pi(s_t,a_t) \\right]\\right].$$\n",
    "\n",
    "We have $J(\\theta')$ as an expression of $A^\\pi$, but we still require sampling according to $\\pi'$. Let us first replace all the $p_t$ with the state occupancy measure:\n",
    "\\begin{align*}\n",
    "J(\\theta') &= J(\\theta) + \\mathbb{E}_{s_t\\sim p_t(\\cdot|\\pi')} \\left[ \\mathbb{E}_{a_t\\sim \\pi'} \\left[ \\sum_{t=0}^\\infty \\gamma^t A^\\pi(s_t,a_t) \\right]\\right]\\\\\n",
    " &= J(\\theta) + \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}_{s_t\\sim p_t(\\cdot|\\pi')} \\left[ \\mathbb{E}_{a_t\\sim \\pi'} \\left[ A^\\pi(s_t,a_t) \\right]\\right] \\\\\n",
    " &= J(\\theta) + \\mathbb{E}_{s\\sim \\rho^{\\pi'}} \\left[ \\mathbb{E}_{a\\sim \\pi'} \\left[ A^\\pi(s,a) \\right]\\right].\n",
    "\\end{align*}\n",
    "\n",
    "Now we would like to sample according to $\\rho^{\\pi}$ and $\\pi$ instead of $\\rho^{\\pi'}$ and $\\pi'$ so we can use importance sampling:\n",
    "$$J(\\theta') = J(\\theta) + \\mathbb{E}_{s\\sim \\rho^{\\pi}} \\left[ \\mathbb{E}_{a\\sim \\pi} \\left[ \\frac{\\rho^{\\pi'}(s)}{\\rho^{\\pi}(s)} \\frac{\\pi'(a|s)}{\\pi(a|s)} A^\\pi(s,a) \\right]\\right].$$\n",
    "\n",
    "This is true as long as the distributions due to $\\pi$ have non-zero mass where the distributions due to $\\pi'$ have non-zero mass.\n",
    "\n",
    "So in theory, we could calculate our policy score $J(\\theta')$ if we knew the ratio $\\frac{\\rho^{\\pi'}(s)}{\\rho^{\\pi}(s)}$. But it is very unclear how to actually compute this ratio.\n",
    "\n",
    "One interesting remark is that neural network updates make very small gradient steps on the policy parameters. So it is likely that between two consecutive gradient steps, the policy does not change much, and hopefully the state distribution does no change much either. In that case, we could neglect this ratio and write $\\frac{\\rho^{\\pi'}(s)}{\\rho^{\\pi}(s)} \\approx 1$, which would enable keeping samples collected by $\\pi$ in memory and re-using them in:\n",
    "$$J(\\theta') \\approx \\mathbb{E}_{s\\sim\\rho^{\\pi}} \\mathbb{E}_{a\\sim \\pi} \\left[ \\frac{\\pi'(a|s)}{\\pi(a|s)}A^\\pi(s,a)\\right].$$\n",
    "\n",
    "**When is $\\frac{\\rho^{\\pi'}(s)}{\\rho^{\\pi}(s)} \\approx 1$?**\n",
    "\n",
    "**The constrained optimization problem and its Lagrangian**\n",
    "\n",
    "**Natural policy gradients**\n",
    "\n",
    "**TRPO**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18138b35-51dc-4f17-98aa-8db737a218d2",
   "metadata": {},
   "source": [
    "## PPO\n",
    "\n",
    "[Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d725ead-ea20-495e-95de-a03c771ce2f9",
   "metadata": {},
   "source": [
    "## Gradient-free policy search\n",
    "\n",
    "[Evolution Strategies as a Scalable Alternative to Reinforcement Learning](https://arxiv.org/abs/1703.03864)  \n",
    "[Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari](https://arxiv.org/abs/1802.08842)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

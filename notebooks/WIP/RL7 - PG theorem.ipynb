{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc11419b-0ffe-4beb-854c-fd0336c1eeb6",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://erachelson.github.io/RLclass_MVA/\">https://erachelson.github.io/RLclass_MVA/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fbc2eb-57c5-4e65-abbe-87dc0bc787d7",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Chapter 6: The policy gradient theorem</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea818474-994d-4898-a8a0-4dff51c899af",
   "metadata": {},
   "source": [
    "So far, we have been mostly concerned with the problem of function approximation, that is \"how do we store $q(s,a)$ in a convenient way, that is amenable to learning, retrieval and optimization?\". A specific feature of the problems we tackled was that they had few discrete actions, making the dependency on $s$ the key difficulty. In particular, when looking for a $q$-greedy action, the $\\max_{a\\in A}$ problem had a straightforward solution as we could iterate through actions and retain the best one. We now turn to a more general case where the actions are too numerous to be enumerated (either because the action space is continuous or because it just has too many actions). Too many states motivated the introduction of function approximators for $v$ and $q$; too many actions similarly lead to function approximation for $\\pi$. In the present chapter, we consider the general policy space (with possible approximation) and ask the question \"how do we find a monotonically improving sequence of policies?\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93c3ea2-ff63-4a30-8a8b-e7a00122f88d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Learning outcomes**   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fdf3f9-2a70-4acd-b0d0-be5dd980fe49",
   "metadata": {},
   "source": [
    "# Policy gradient methods\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Bottomline question:**   \n",
    "The previous chapters have focussed on *action-value methods*; they aimed at estimating $q^*$ in order to deduce $\\pi^*$, or they jointly optimized $q$ and $\\pi$. Could we directly optimize $\\pi$?\n",
    "</div>\n",
    "\n",
    "Suppose we have a policy $\\pi_\\theta$ parameterized by a vector $\\theta$. Our goal is to find the parameter $\\theta^*$ corresponding to $\\pi^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc535ba-951e-4c7a-ae5d-ba6ab684b299",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Recall the FrozenLake environment.  \n",
    "How many states and how many actions were there in this environment? \n",
    "What would be a policy parameterization which does not make any approximation (ie. that can represent any policy in the policy space) for stationary, memoryless, stochastic policies?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472994ab-5894-4a98-89bb-5c00bff2b430",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "There are 16 states and 4 actions in the FrozenLake game.\n",
    "\n",
    "A stationary, memoryless, stochastic policy is a mapping from $S$ to $\\Delta_A$. Since $S$ and $A$ are discrete, it can be represented in tabular form as:\n",
    "$$\\pi = \\left[ \\begin{array}{cccc}\n",
    "\\pi(a_0|s_0) & \\pi(a_1|s_0) & \\pi(a_2|s_0) & 1 - \\sum_{i=0}^2 \\pi(a_i|s_0) \\\\\n",
    "\\ldots & & & \\\\\n",
    "\\pi(a_0|s_{|S|}) & \\pi(a_1|s_{|S|}) & \\pi(a_2|s_{|S|}) & 1 - \\sum_{i=0}^2 \\pi(a_i|s_{|S|})\n",
    "\\end{array} \\right].$$\n",
    "\n",
    "This parameterization enables representing any stochastic policy. It involves $|A|-1=3$ parameters per line, and $|S|=16$ lines, so in total $3\\times 16 = 58$ parameters.\n",
    "\n",
    "As previously, parameterization does not necessarily involve approximation! As the action set will become large or continuous, parameterization will enable generalization across actions at the cost of approximation, but a tabular representation is also a policy parameterization.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77337f57-75e3-43c3-b036-1edb73dcde69",
   "metadata": {},
   "source": [
    "Remarks:\n",
    "- $\\pi_\\theta$ might not be able to represent $\\pi^*$. We will take a shortcut and call $\\pi^*$ the best policy among the $\\pi_\\theta$ ones.\n",
    "- For discrete state and action spaces, the tabular policy representation is a special case of policy parameterization.\n",
    "- Policy parameterization is a (possibly useful) way of introducing prior knowledge on the set of the desired policies.\n",
    "- The optimal deterministic policies might not belong to the policy subspace of $\\pi_\\theta$, thus it makes sense to consider stochastic policies for $\\pi_\\theta$.\n",
    "- It makes even more sense to consider stochastic policies that it opens the family of environments that we can tackle, like partially observable MDPs or multi-player games.\n",
    "\n",
    "For stochastic policies, we shall write $\\pi_\\theta(a|s)$.\n",
    "\n",
    "In the remainder of the chapter, we will assume that $\\pi_\\theta(a|s)$ is differentiable with respect to $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0164591c-c6ea-4766-80b5-838c76a93d52",
   "metadata": {},
   "source": [
    "Suppose now we define some performance metric $J(\\pi_\\theta) = J(\\theta)$. If $J$ is differentiable and a stochastic estimate $\\tilde{\\nabla}_\\theta J(\\theta)$ of the gradient is available, then we can define the stochastic gradient ascent update procedure:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\tilde{\\nabla}_\\theta J(\\theta).$$\n",
    "\n",
    "We will call **policy gradient methods** all methods that follow such a procedure (whether or not they also learn a value function or not).\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Policy gradient method**   \n",
    "We call **policy gradient method** any method that performs stochastic gradient ascent on the policy's parameters.  \n",
    "Given a stochastic estimate $\\tilde{\\nabla}_\\theta J(\\theta)$ of a policy's performance criterion with respect to the policy's parameters, such a method implements the update procedure: \n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\tilde{\\nabla}_\\theta J(\\theta).$$\n",
    "</div>\n",
    "\n",
    "Remarks: \n",
    "- Note that $J$ is a generic criterion. For example, $J$ could be defined as the $\\gamma$-discounted value of a starting state (or a distribution of starting states), or as the undiscounted reward over a certain horizon, or as the average reward.\n",
    "- Note that this family of methods can use any gradient estimate for $\\tilde{\\nabla}_\\theta J(\\theta)$: formal calculus, finite differences, automated differentiation, evolution strategies, etc.\n",
    "- Why is it interesting to look at methods which explicitly store a policy function? Because the evaluation of the policy in a given state $s$ does not require the maximization step ($\\max_a Q(s,a)$), which might be computationally costly, especially for continuous actions. Instead, it replaces it with a call to $\\pi_\\theta(s)$ (or a draw from $\\pi_\\theta(a|s)$). This argument makes actor-critic architectures or direct policy search a method of choice for continuous actions domains (especially common in Robotics) and Policy Gradient is one of them.\n",
    "- When do policy gradient approaches outperform value-based ones? It's hard to give a precise criterion; it really depends on the problem. One thing that comes into play is how easy it is to approximate the optimal policy or the optimal value function. If one is simpler than the other (by \"simpler\", we mean \"it is easier to find a parameterization whose spanned function space almost includes the function to approximate\"), then it is a good heuristic to try to approximate it. But this criterion might itself be hard to assess."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a90f48-67fc-44c5-82c5-3fbf56199aaa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Exercise**  \n",
    "From the class on Markov Decision Processes, can you recall a scalar criterion $J(\\pi)$ whose optimization is provably equivalent to finding a policy that dominates any other one in every state?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f56515-4ab6-4475-a249-fb32a6e11542",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Provided $\\rho_0$ has non-zero probability mass on all states, an optimal policy is a solution to $\\max_\\pi J(\\pi) = \\mathbb{E}_{s_0\\sim \\rho_0}[V^\\pi(s_0)]$.\n",
    "\n",
    "This makes optimizing $J(\\pi)$ a legitimate goal for finding optimal policies: from now on we will work with $J(\\pi)$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d122f745-2e58-42eb-a124-efa9a0d767f9",
   "metadata": {},
   "source": [
    "**Notations**\n",
    "\n",
    "- We consider probability density functions $p(X)$ for all random variables $X$.\n",
    "- For a policy $\\pi_\\theta$ and a random variable $X$ we write indifferently $p(X|\\pi_\\theta) = p(X|\\theta)$.\n",
    "- We will write $\\pi_\\theta(s)$ the policy's distribution over actions in $s$, and $\\pi_\\theta(a|s)$ the probability that this policy picks action $a$ in $s$.\n",
    "- A trajectory is noted $\\tau = (s_t,a_t)_{t\\in \\mathbb{N}}$.\n",
    "- The state random variable at step $t$ is $S_t$ and its law's density is $p_t(s)$.\n",
    "- The action random variable at step $t$ is $A_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1698c3c8-d51a-4e27-b525-eb6168170c12",
   "metadata": {},
   "source": [
    "This very short chapter serves as a header to two families of estimators for policy gradients, which we will study separately.  \n",
    "First, we will look at Monte Carlo policy gradient methods. These methods rely on simulating one or several full trajectories with $\\pi$ to compute $\\nabla J$. These methods include the very well-known REINFORCE method, but also most of the literature on evolutionary reinforcement learning.  \n",
    "Then, we will follow the same kind of reasoning the enabled moving from $v(s) = \\sum_t \\gamma^t r_t$ to the Bellman equation, and will study how $\\nabla J$ can be expressed as a quantity one can estimate from independent $(s,a,r,s')$ samples. All the corresponding algorithms will be motivated by the famous *policy gradient theorem*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defc71e5-d5fd-4d7d-8046-48fbc361ab4c",
   "metadata": {},
   "source": [
    "# Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ccea04-1c19-45e8-830f-6d71d071e12c",
   "metadata": {},
   "source": [
    "Before we start, let's warm up with three key reminders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c829a7-7770-452b-8a6b-ef98adc56d99",
   "metadata": {},
   "source": [
    "## Reminder 1: the policy improvement theorem\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Policy improvement theorem**  \n",
    "If $\\pi_{n+1} \\in \\mathbb{G}q^{\\pi_n}$, then $q^{\\pi_{n+1}} \\geq q^{\\pi_n}$.\n",
    "</div>\n",
    "\n",
    "As stated in the introduction, we are searching for a monotonously improving sequence of policies. This is exactly what the policy improvement theorem provided: if one knows $q^{\\pi_0}$, then taking $\\pi_1 \\in \\mathbb{G}q^{\\pi_0}$ yields a policy that is no worse than $\\pi_0$. Previously, this lead us to the policy iteration algorithm. Now, finding a greedy policy with respect to $q^{\\pi_0}$ might not be that easy. Gradient ascent seems like a reasonable perspective from that point of view. So we already get a feeling there will be a connection between finding a improving sequence of policies and finding a greedy policy with respect to some value function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b0979-cdd4-4fe0-b909-b09be8a1bbc7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise (easy misconception):</b><br>\n",
    "Policy iteration yields a monotonically improving sequence of value functions. Can we say the same thing about value iteration or modified policy iteration?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad8a75-21c5-48f2-a61a-46a86c51d537",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "Interestingly, no. To obtain the guarantee that $q^{\\pi_1}$ is better than $q^{\\pi_0}$, one really needs to define $\\pi_1$ as greedy with respect to $q^{\\pi_0}$.  \n",
    "Making it greedy with respect to $(\\mathbb{T}^{\\pi_0})^m q$ (for some initial $q$) is not sufficient.  \n",
    "Funny, isn't it? We cannot guarantee a priori that the sequence of policies provided by DP methods is of increasing quality (beside the case of policy iteration). It still converges to $\\pi^*$, but monotonicity is not guaranteed (at least not through the policy improvement theorem).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126c538e-c773-4e50-9e09-68dc8e4e9b1a",
   "metadata": {},
   "source": [
    "## Reminder 2: the linearity of the Bellman evaluation equation\n",
    "\n",
    "Let us write $p^\\pi(s'|s)$ the probability density of the transition kernel of the Markov chain corresponding to our MDP, controlled by $\\pi$. In other words $p^\\pi(s'|s) = \\int_A \\pi(a|s) p(s'|s,a) da$.\n",
    "\n",
    "The transition kernel $p^\\pi$ is a linear operator that transforms a value function into another:\n",
    "$$(p^\\pi v)(s) = \\int_S v(s') p^\\pi(s'|s) ds'.$$\n",
    "\n",
    "Similarly, let us write $r^\\pi = \\int_A \\pi(a|s) r(s,a) da$.\n",
    "\n",
    "With these remarks, one can rewrite the Bellman equation:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Bellman equation (evaluation)**  \n",
    "$v^\\pi$ is the unique solution to $v = \\mathbb{T}^\\pi v$ with\n",
    "$$\\mathbb{T}^\\pi v = r^\\pi + \\gamma p^\\pi v.$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4fa71c-cb55-4c9b-8c23-c7e230b03cb7",
   "metadata": {},
   "source": [
    "Note that, in finite dimension, writing that $p^\\pi$ is a linear operator is just an application of the equivalence between the matrix of an operator and the operator itself. This is particularly useful to simplify notations because expectations can then be written as inner products:\n",
    "$$\\mathbb{E}_{s'} [v(s')] = \\int_S v(s') p^\\pi(s'|s) ds' = (p^\\pi v)(s).$$\n",
    "But this applies also to other linear operators defined from probability maps:\n",
    "$$v^\\pi(s) = \\mathbb{E}_{a\\sim \\pi} [q^\\pi(s,a)] = \\int_A q^\\pi(s,a) \\pi(a|s) da = (\\pi q^\\pi)(s).$$\n",
    "So we can write, in short:\n",
    "$$v^\\pi = \\pi q^\\pi.$$\n",
    "All in all, this boils down to writing products between linear operators (and products between matrices when $\\mathcal{S}$ and $\\mathcal{A}$ are finite dimensional).  \n",
    "These notations are much lighter than our previous ones, and remain true whatever the nature of $\\mathcal{S}$ and $\\mathcal{A}$ (finite or not). They are somehow less intuitive than writing full expectations, but help make equations much more readable and will come in handy in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d2e39-4371-45c8-8d03-98fa25575956",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "    \n",
    "Note that the transition kernel $p(s'|s,a)$ is a linear operator too. Rewrite the Bellman evaluation equation on state-action value functions, using the notations above. Try playing a bit with the notation, by composing $\\pi$ and $p$ operators to rewrite other versions of the Bellman equation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb7e9ea-ae16-409e-9659-d89954771d11",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "$p$ is a linear operator such that $(p v)(s,a) = \\int_S v(s') p(s'|s,a) ds'$.  \n",
    "It transforms a function $\\mathcal{S}\\rightarrow \\mathbb{R}$ into another function $\\mathcal{S}\\times\\mathcal{A} \\rightarrow \\mathbb{R}$.  \n",
    "In finite dimension, it can be represented by an $|\\mathcal{S}||\\mathcal{A}|\\times|\\mathcal{S}|$ matrix.\n",
    "\n",
    "The Bellman equation on state-actions value functions is:\n",
    "$$(\\mathbb{T}^\\pi q)(s,a) = r(s,a) + \\gamma \\int_S \\left[\\int_A \\pi(a'|s') q(s',a') da'\\right] p(s'|s,a) ds'.$$\n",
    "That is:\n",
    "$$(\\mathbb{T}^\\pi q)(s,a) = r(s,a) + \\gamma \\int_S (\\pi q)(s') p(s'|s,a) ds'.$$\n",
    "And so:\n",
    "$$(\\mathbb{T}^\\pi q)(s,a) = r(s,a) + \\gamma (p (\\pi q))(s,a).$$\n",
    "Turning back to the general function form yields:\n",
    "$$\\mathbb{T}^\\pi q = r + \\gamma p \\pi q$$\n",
    "\n",
    "Note that identifying the matrix corresponding to the $\\pi$ operator in finite dimension is a bit tricky and requires playing with matrix dimensions appropriately. We won't go any further into details here, the point of this exercise was just to introduce the notation.\n",
    "\n",
    "Just playing with the notation, we have:\n",
    "$$(p^\\pi v)(s) = \\int_S v(s') p^\\pi(s'|s) ds' = \\int_A \\int_S v(s') p(s'|s,a) ds' \\pi(a|s) da = (\\pi p v)(s),$$\n",
    "$$p^\\pi v = \\pi p v.$$\n",
    "Since one has $r^\\pi = \\pi r$, the Bellman equation on state value functions becomes:\n",
    "$$v = \\pi (r + \\gamma p v).$$\n",
    "\n",
    "Note that we used $r$ as an $r(s,a)$ function above.\n",
    "\n",
    "The take-away message of this exercise is really that the evaluation equation can be cast in the simple form of composition of linear operators, which makes notations a lot easier to manipulate.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99513a43-10b9-4cb1-9270-6416f920dd54",
   "metadata": {},
   "source": [
    "## Reminder 3: the state occupation measure\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**The state occupation measure and the scalar value of a policy**  \n",
    "$$J(\\pi) = \\mathbb{E}_{(s_i,a_i)_{i \\in \\mathbb{N}}} \\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)  | \\pi, p_0 \\right] = \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ r(s,a) \\right]$$\n",
    "with $\\rho^\\pi(s) = \\sum_{t=0}^\\infty \\gamma^t p_t(s|\\pi)$ is the *state occupancy measure under policy $\\pi$ and starting distribution $p_0$*.\n",
    "</div>\n",
    "\n",
    "Note that $\\rho^\\pi$ is not a proper distribution per se (it sums to $\\frac{1}{1-\\gamma}$), and is sometimes also called the *improper state distribution under policy $\\pi$* or the *improper state visitation frequency under policy $\\pi$*, so we cannot really take an expectation given that $s$ is drawn according to $\\rho^\\pi$. What we wrote above is a slight notation abuse for:\n",
    "$$J(\\pi) = \\int_S \\left[ \\int_A r(s,a) \\pi (a|s) da \\right] \\rho^\\pi(s) ds.$$\n",
    "\n",
    "One could easily normalize $\\rho^\\pi$ to make it a proper probability density function. This would only introduce a constant $(1-\\gamma)$ multiplicative factor to the scalar criterion $J(\\pi)$ above.\n",
    "\n",
    "In plain words, the value of a policy $\\pi$ is the average value of the rewards when states are sampled according to $\\rho^\\pi$ and actions are sampled according to $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd68105c-eb64-4c3a-8430-743e07ebfda2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "Recall how one proves this result.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee93ad8-f32c-40e7-88ff-594178ef6d06",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "One has:\n",
    "$$J(\\pi) = \\mathbb{E}_{(s_i,a_i)_{i \\in \\mathbb{N}}} \\left[ \\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)  | \\pi, p_0 \\right].$$\n",
    "\n",
    "In the sum above:\n",
    "- $s_0$ is drawn according to $p_0$,\n",
    "- $a_0$ is drawn according to $\\pi(s_0)$,\n",
    "- all subsequent $s_{t+1}$ are drawn according to $p(s_{t+1}|s_t,a_t)$,\n",
    "- and all subsequent actions $a_{t+1}$ are drawn according to $\\pi(s_{t+1})$.\n",
    "\n",
    "To facilitate the reading, we will drop the mention to $p_0$ in what follows.\n",
    "We can switch the sum and the expectation and get:  \n",
    "$$J(\\pi) = \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}_{(s_i,a_i)_{i \\in \\mathbb{N}}} \\left[ r(s_t,a_t)  | \\pi \\right].$$\n",
    "But $\\mathbb{E}_{(s_i,a_i)_{i \\in \\mathbb{N}}} \\left[ r(s_t,a_t)  | \\pi \\right] = \\mathbb{E}_{s_t,a_t} \\left[ r(s_t,a_t)  | \\pi \\right]$. So:\n",
    "$$J(\\pi) = \\sum_{t=0}^\\infty \\gamma^t \\mathbb{E}_{s_t,a_t} \\left[ r(s_t,a_t)  | \\pi \\right].$$\n",
    "Now let's introduce the density of $(s_t,a_t)$:\n",
    "$$J(\\pi) = \\sum_{t=0}^\\infty \\gamma^t \\int_S \\int_A r(s_t,a_t) p(s_t,a_t|\\pi) ds_t da_t.$$\n",
    "But $p(s_t,a_t|\\pi) = p(s_t|\\pi) p(a_t|s_t,\\pi)$. By definition, $p(s_t|\\pi) = p_t(s|\\pi)$ and $p(a_t=a|s_t=s,\\pi) = \\pi(a|s)$. So:\n",
    "$$J(\\pi) = \\sum_{t=0}^\\infty \\gamma^t \\int_S \\int_A r(s,a) p_t(s|\\pi) \\pi(a|s) ds da.$$\n",
    "Let us isolate the terms that concern only states:\n",
    "$$J(\\pi) = \\int_S \\left[ \\int_A r(s,a) \\pi(a|s) da \\right] \\sum_{t=0}^\\infty \\gamma^t p_t(s|\\pi) ds.$$\n",
    "By noting $\\rho^\\pi(s) = \\sum_{t=0}^\\infty \\gamma^t p_t(s|\\pi)$ we obtain:\n",
    "$$J(\\pi) = \\int_S \\left[ \\int_A r(s,a) \\pi (a|s) da \\right] \\rho^\\pi(s) ds.$$\n",
    "And so finally, with a slight notation abuse because $\\rho^\\pi$ is not a probability distribution:\n",
    "$$J(\\theta) = \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ r(s,a) \\right].$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f26ce-40fd-48d1-a8f5-8e912e12c2ae",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"><b>Exercise:</b><br>\n",
    "In the second reminder, we introduced $p^\\pi$ as the operator which transformed a function $\\mathcal{S}\\rightarrow\\mathbb{R}$ into another one, such that $(p^\\pi v)(s) = \\int_S v(s') p^\\pi(s'|s) ds'$. Use this to write $\\rho^\\pi$ using $p^\\pi$ and $\\rho_0$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263f2891-ea61-4b20-bffe-6169841d9826",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "One has $p_t(s|\\pi) = \\rho_0 (p^\\pi)^t$.  \n",
    "So $\\rho^\\pi = \\rho_0 \\sum_t (\\gamma p^\\pi)^t$.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa3d439-2015-4152-ac30-0689aaaa950f",
   "metadata": {},
   "source": [
    "# The policy gradient theorem\n",
    "\n",
    "In this section, we derive our first key result in this class: can we obtain a usable expression for $\\nabla_\\theta J(\\theta)$, so that we can take gradient steps which improve the current policy (hence the name of this whole chapter: policy improving gradients)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b58f2-29d7-4d0f-83c7-57cf779b55da",
   "metadata": {},
   "source": [
    "## A Bellman equation on value gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87c9026-2541-47f1-9807-fd5b0e230548",
   "metadata": {},
   "source": [
    "As indicated in the previous section, we want to optimize the scalar criterion \n",
    "$$J(\\theta) = \\mathbb{E}_{s_0\\sim\\rho_0} [v^{\\pi_\\theta}(s_0)].$$\n",
    "\n",
    "So, quite immediately, \n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s_0\\sim\\rho_0} [\\nabla_\\theta v^{\\pi_\\theta}(s_0)].$$\n",
    "\n",
    "Let us look a little into $\\nabla_\\theta V^{\\pi_\\theta}(s_0)$. We have \n",
    "$$v^\\pi(s) = \\mathbb{E}_{a\\sim \\pi} [q^\\pi(s,a)] = \\int_A q^\\pi(s,a) \\pi(a|s) da.$$\n",
    "\n",
    "So \n",
    "$$\\nabla_\\theta v^\\pi(s) = \\int_A \\Big[q^\\pi(s,a) \\nabla_\\theta \\pi(a|s) + \\pi(a|s) \\nabla_\\theta q^\\pi(s,a)\\Big] da.$$\n",
    "\n",
    "Now, using the definition of $q^\\pi$, \n",
    "$$\\nabla_\\theta v^\\pi(s) = \\int_A \\Big[q^\\pi(s,a) \\nabla_\\theta \\pi(a|s) + \\pi(a|s) \\nabla_\\theta \\int_S (r(s,a,s') + \\gamma v^\\pi(s')) p(s'|s,a) ds'\\Big] da.$$\n",
    "\n",
    "Quite obviously, $\\nabla_\\theta r(s,a,s') = 0$. So we obtain\n",
    "$$\\nabla_\\theta v^\\pi(s) = \\int_A \\Big[q^\\pi(s,a) \\nabla_\\theta \\pi(a|s) +  \\gamma \\pi(a|s) \\nabla_\\theta \\int_S v^\\pi(s') p(s'|s,a) ds'\\Big] da.$$\n",
    "\n",
    "We can split this in two parts, and switch the integration order in the second term. This yields\n",
    "$$\\nabla_\\theta v^\\pi(s) = \\int_A q^\\pi(s,a) \\nabla_\\theta \\pi(a|s) da +  \\gamma  \\int_S \\nabla_\\theta v^\\pi(s') \\Big[\\int_A \\pi(a|s) p(s'|s,a) da \\Big] ds'.$$\n",
    "\n",
    "Let us write $p^\\pi(s'|s) = \\int_A \\pi(a|s) p(s'|s,a) da$. It is the transition kernel of the Markov chain defined by the MDP controled by $\\pi$. We have:\n",
    "$$\\nabla_\\theta v^\\pi(s) = \\int_A q^\\pi(s,a) \\nabla_\\theta \\pi(a|s) da +  \\gamma  \\int_S \\nabla_\\theta v^\\pi(s') p^\\pi(s'|s) ds'.$$\n",
    "\n",
    "Swithcing back to an expectation over $s'$, we have:\n",
    "$$\\nabla_\\theta v^\\pi(s) = \\int_A q^\\pi(s,a) \\nabla_\\theta \\pi(a|s) da + \\gamma \\mathbb{E}_{s'\\sim p^\\pi(s'|s)} [\\nabla_\\theta v^\\pi(s')].$$\n",
    "\n",
    "This is a Bellman equation on $\\nabla_\\theta V^\\pi$. The first terms acts as a one-step reward, and the second is the expectation over next states of $\\nabla v^\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642002f9-092e-4d76-94be-e2b652d8200e",
   "metadata": {},
   "source": [
    "We used the $v^\\pi(s)$, $q^\\pi(s,a)$, etc. notations in order remain didactic, but the writing above is quite ugly. Let us turn to function notation and operator products, as in the warm-up exercises, and re-write the same thing:\n",
    "$$v^\\pi = \\pi q^\\pi$$\n",
    "Let us drop the $\\theta$ subscript in $\\nabla_\\theta$. Then (with a slight notation abuse on the order inside the products, to keep things readable):\n",
    "\\begin{align*}\n",
    "    \\nabla v^\\pi &= \\nabla (\\pi q^\\pi)\\\\\n",
    "    &= q^\\pi \\nabla \\pi + \\pi \\nabla q^\\pi\\\\\n",
    "    &= q^\\pi \\nabla \\pi + \\pi \\nabla (r + \\gamma p v^\\pi)\\\\\n",
    "    &= q^\\pi \\nabla \\pi + \\gamma \\pi p\\nabla v^\\pi\\\\\n",
    "    &= q^\\pi \\nabla \\pi + \\gamma p^\\pi \\nabla v^\\pi\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ae34e-e587-461e-8591-ef7f68babbfd",
   "metadata": {},
   "source": [
    "This yields a nice Bellman equation on the value function gradient:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Bellman equation on $\\nabla v^\\pi$**  \n",
    "$$\\nabla v^\\pi = q^\\pi \\nabla \\pi + \\gamma p^\\pi \\nabla v^\\pi$$\n",
    "where $\\nabla$ is the gradient operator, with respect to the policy parameters. In expanded notation:\n",
    "\n",
    "$$\\nabla_\\theta v^\\pi(s) = \\int_A q^\\pi(s,a) \\nabla_\\theta \\pi(a|s) da + \\gamma \\mathbb{E}_{s'\\sim p^\\pi(s'|s)} [\\nabla_\\theta v^\\pi(s')].$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cbc531-aad1-4ce4-990d-a45fa1384bc0",
   "metadata": {},
   "source": [
    "## The policy gradient theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35311e5-6357-4fbc-8881-97a8175441c2",
   "metadata": {},
   "source": [
    "As with the previous Bellman equations we studied, this one too has a fixed point, corresponding to the infinite sum of discounted rewards, that is:\n",
    "$$\\nabla v^\\pi = \\sum_{t=0}^\\infty \\gamma^t (p^\\pi)^t (q^\\pi \\nabla \\pi).$$\n",
    "\n",
    "Recall the definition of the state occupation measure $\\rho^\\pi = \\rho_0 \\sum_{t=0}^\\infty \\gamma^t (p^\\pi)^t$. Then we immediately have:\n",
    "$$\\nabla_\\theta J(\\theta) = \\rho^\\pi (q^\\pi \\nabla \\pi)$$\n",
    "\n",
    "Let us make this explicit to understand how we can obtain an estimator of this gradient:\n",
    "\\begin{align*}\n",
    "\\nabla_\\theta J(\\theta) &= \\int_S (q^\\pi \\nabla \\pi)(s) \\rho^\\pi(s) ds\\\\\n",
    "                        &= \\int_S \\left[ \\int_A q^\\pi(s,a) \\nabla \\pi(a|s) da \\right] \\rho^\\pi(s) ds\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Let us suppose we can draw samples from $\\rho^\\pi$, for example by supposing that a replay buffer will mimic $\\rho^\\pi$ (note that this assumption's validity is far from obvious, but we shall make it anyway). We could have a Monte Carlo estimator of the policy gradient if we had a Monte Carlo estimator of the term inside the brackets.\n",
    "\n",
    "Let us remark that:\n",
    "\\begin{align*}\n",
    "\\int_A q^\\pi(s,a) \\nabla \\pi(a|s) da &= \\int_A q^\\pi(s,a) \\frac{\\nabla \\pi(a|s)}{\\pi(a|s)} \\pi(a|s) da \\\\\n",
    " &= \\int_A q^\\pi(s,a) \\nabla \\log \\pi(a|s) \\pi(a|s) da\\\\\n",
    " &= \\mathbb{E}_{a \\sim \\pi(s)} \\left[ q^\\pi(s,a) \\nabla \\log \\pi(a|s) \\right]\n",
    "\\end{align*}\n",
    "\n",
    "We have turned our integral into an expectation. This is sometimes called the *nabla-log* trick.\n",
    "\n",
    "And finally we have expressed the gradient of $J(\\theta)$ as directly proportional to the value of $q^\\pi$ and the gradient of $\\log\\pi$:\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Policy gradient theorem:**  \n",
    "$$\\nabla_\\theta J(\\theta) \\propto \\mathbb{E}_{\\substack{s\\sim\\rho^\\pi \\\\ a\\sim \\pi}} \\left[ q^\\pi(s,a) \\nabla_\\theta \\log\\pi(a|s)\\right]$$\n",
    "\n",
    "Or in simpler notation:\n",
    "$$\\nabla_\\theta J(\\theta) = \\rho^\\pi \\pi (q^\\pi \\nabla \\log \\pi)$$\n",
    "</div>\n",
    "\n",
    "The interpretation of this theorem is very straightforward:  \n",
    "To increase the average value of policy $\\pi_\\theta$ over the starting distribution $p_0$, we should change $\\theta$ in a direction that is a linear combination of the $\\nabla_\\theta \\log \\pi(a|s)$, where the coefficients are the expected outcomes $q^\\pi(s,a)$ of picking action $a$ in $s$.  \n",
    "Since $\\nabla_\\theta \\log \\pi(a|s)$ is a direction that increases the log probability of $a$ in $s$, we can rephrase the last sentence. The policy gradient tells us:  \n",
    "**To increase the value of the current policy, we should increase the log-probability of $a$ in $s$ in proportion to the expected outcome of $a$ in $s$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f8f452-8c79-4ac2-8396-cc661856523a",
   "metadata": {},
   "source": [
    "## The link with $q$-greedy actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904fb7cc-50a1-4b56-83d7-5493ed87cce8",
   "metadata": {},
   "source": [
    "We can now connect the policy gradient we just wrote, with the search for policies that are greedy with respect to some value function $q$.\n",
    "\n",
    "It is quite tempting, when one has a value function $q$ and a policy $\\pi$, to try to compute $\\nabla_\\theta \\mathbb{E}_{s\\sim\\rho} [q(s,\\pi(s))]$ given that we have states sampled from some distribution $\\rho$. The question we would like to answer is:  \n",
    "**Under what conditions does this *greediness gradient* match the true policy gradient that enables finding a policy that is better than $\\pi$?**\n",
    "\n",
    "Let us simply rewrite our greediness gradient:\n",
    "$$\\nabla_\\theta \\mathbb{E}_{s\\sim\\rho} [q(s,\\pi(s))] = \\nabla \\rho (\\pi q)$$\n",
    "And since $\\rho$ and $q$ do not depend on $\\theta$ (with the same slight notation abuse as before on the order inside the product):\n",
    "$$\\nabla_\\theta \\mathbb{E}_{s\\sim\\rho} [q(s,\\pi(s))] = \\rho (q \\nabla \\pi)$$\n",
    "As before, one can turn this into a gradient that can be estimated via sampling, using the nabla-log trick:\n",
    "$$\\nabla_\\theta \\mathbb{E}_{s\\sim\\rho} [q(s,\\pi(s))] = \\rho \\pi (q \\nabla \\log \\pi)$$\n",
    "\n",
    "This expression is equal to the true policy gradient if $\\rho=\\rho^\\pi$ and if $q = q^\\pi$.\n",
    "\n",
    "In the end, **this is a generalization of the policy improvement theorem**:  \n",
    "if $q=q^\\pi$ (just as in the policy improvement theorem),  \n",
    "and if $s$ is sampled according to $\\rho^\\pi$,  \n",
    "then the direction $\\nabla_\\theta \\mathbb{E}_{s\\sim\\rho}[q(s,\\pi(s)] = \\mathbb{E}_{\\substack{s\\sim\\rho\\\\ a\\sim\\pi}} [q(s,a) \\nabla_\\theta \\log \\pi(a|s)]$,  \n",
    "is an ascent direction for $J(\\pi)$.  \n",
    "So the sequence of policies following infinitesimal increments along this greediness gradient follows a monotonous improvement in value.\n",
    "\n",
    "Conversely, if $q \\neq q^\\pi$ and if $\\rho \\neq \\rho^\\pi$ we can't guarantee anything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03ac2aa-da8f-4637-9c59-eabe188a975a",
   "metadata": {},
   "source": [
    "## Deterministic policies: a limit case\n",
    "\n",
    "Left out for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c557b4-8698-444f-9b3b-fb3b07c6d310",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a202f-a22e-4b2f-838a-7f80fc687fb0",
   "metadata": {},
   "source": [
    "# A roll-out based view on policy gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab538eb-26ea-4d1d-bd88-f5774501fdbd",
   "metadata": {},
   "source": [
    "## A Monte-Carlo policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfac6dd-c3c7-4e72-893b-77ebbda6e771",
   "metadata": {},
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5098bc-4934-4b4e-a3b7-31fd225a2340",
   "metadata": {},
   "source": [
    "# Actor-critic algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884e6a56-f2aa-4fbe-88ad-1df11c701ad0",
   "metadata": {},
   "source": [
    "## Introducing a critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4885dd7-8a76-4600-9a8f-27190d6ea1d3",
   "metadata": {},
   "source": [
    "## Baselines in policy gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee54310-ff34-4610-879c-dcd6fc3d683a",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1acf267-03ce-475a-86e7-2ca46999cf01",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e4ea2-e29b-4763-a58e-642fdb655fb5",
   "metadata": {},
   "source": [
    "- DDPG\n",
    "- TD3\n",
    "- SAC\n",
    "- A2C\n",
    "- Running rollouts in parallel\n",
    "- PG on continuous action domains\n",
    "- PG for the finite horizon criterion\n",
    "- GAE\n",
    "- From off-policy PG to TRPO\n",
    "- PPO\n",
    "- Gradient free policy search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
